{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e9e591",
   "metadata": {},
   "source": [
    "# ASSIGNMENT REPORT NOTEBOOK\n",
    "\n",
    "| Môn học | **Cơ sở Toán cho Khoa học Máy tính (Mathematics Fundamentals for Computer Science)** |\n",
    "| - | - |\n",
    "| Mã môn học | **CO5097** |\n",
    "| Giảng viên | **Thầy Nguyễn An Khương** |\n",
    "| Nhóm | **Optimization Momentum** |\n",
    "\n",
    "\n",
    "#### Danh sách thành viên:\n",
    "\n",
    "| # | Họ và tên | MSHV |\n",
    "| - | - | - |\n",
    "| 1 | Lê Thái Bình | 2570081 |\n",
    "| 2 | Ngô Duy Bảo | 2570395 |\n",
    "| 3| Trần Thái Học | 2570426 |\n",
    "| 4 | Tiêu Trí Kiệt | 2570436 |\n",
    "\n",
    "\n",
    "#### Nội dung tham khảo:\n",
    "\n",
    "- [12.6 Momentum](d2l.ai/chapter_optimization/momentum.html)\n",
    "- [Bài 8 Gradient Descent (phần 2/2)](https://machinelearningcoban.com/2017/01/16/gradientdescent2/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7660bcb",
   "metadata": {},
   "source": [
    "## I. Giới thiệu\n",
    "---\n",
    "\n",
    "### **I.1. Phát biểu bài toán (Problem Statement)**\n",
    "\n",
    "Trong học sâu, ta thường cần tìm bộ tham số $w$ tối ưu để giảm thiểu một hàm mất mát: $min_{w}f(\\mathbf{w})$. <br>\n",
    "\n",
    "Gradient descent cập nhật tham số theo: $\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla f(\\mathbf{w})$.<br>\n",
    "\n",
    "Tuy nhiên, gradient descent thường gặp các vấn đề:\n",
    "\n",
    "- **Hội tụ chậm**, đặc biệt ở các vùng rãnh dốc (ravines) hoặc nơi đạo hàm thay đổi mạnh theo từng chiều.\n",
    "- **Dễ bị dao động** (oscillation) khi gradient ở hướng này rất lớn nhưng ở hướng kia rất nhỏ.\n",
    "- Khó thoát khỏi các vùng “bằng phẳng” (flat regions).\n",
    "\n",
    "Ví dụ:\n",
    "\n",
    "\\begin{aligned}\n",
    "f(x,y) = 0.1x^2 + y^2 \\tag{1.1}\n",
    "\\end{aligned}\n",
    "\n",
    "![SGD vs Momentum](./figure_1.png)\n",
    "\n",
    "![SGD vs Momentum on Flat Region](./figure_2.png)\n",
    "\n",
    "Hoặc hàm như sau:\n",
    "\n",
    "\\begin{aligned}\n",
    "f(x) = 0.1x^2 + 0.001x^4 \\tag{1.2}\n",
    "\\end{aligned}\n",
    "\n",
    "![SGD vs Momentum on Flat Region](./figure_3.png)\n",
    "\n",
    "Để giải quyết các nhược điểm này, ta sử dụng **Momentum**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938284e1",
   "metadata": {},
   "source": [
    "## I.2 CÁC BÀI TOÁN  \n",
    "\n",
    "\n",
    "## I.2.1 Leaky Averages\n",
    "\n",
    "### Nhắc lại kiến thức về Gradient Descent và SGD Minibatch\n",
    "\n",
    "Theo Gradient Descent, ta có:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g}_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_t = \\partial_{\\mathbf{w}} f(\\mathbf{x}_{t}, \\mathbf{w})\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $\\mathbf{w}$: tham số mô hình  \n",
    "- $\\eta_t$: Learning rate  \n",
    "- $\\mathbf{g}_t$: Gradient của hàm loss function tại thời điểm t  \n",
    "- $\\mathbf{x}_t$: Dữ liệu đầu vào  \n",
    "- $f(\\mathbf{x}_{t}, \\mathbf{w})$: hàm loss khi dự đoán trên mẫu \\(x_t\\)  \n",
    "- $\\partial_{\\mathbf{w}}$: đạo hàm riêng theo w  \n",
    "\n",
    "\n",
    "\n",
    "### Minibatch SGD\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_{t, t-1}\n",
    "= \\partial_{\\mathbf{w}} \n",
    "  \\frac{1}{|\\mathcal{B}_t|}\n",
    "  \\sum_{i \\in \\mathcal{B}_t} \n",
    "  f(\\mathbf{x}_{i}, \\mathbf{w}_{t-1})\n",
    "= \\frac{1}{|\\mathcal{B}_t|}\n",
    "  \\sum_{i \\in \\mathcal{B}_t} \n",
    "  \\mathbf{h}_{i, t-1}\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $\\mathbf{h}_{i, t-1} = \\partial_{\\mathbf{w}} f(\\mathbf{x}_i, \\mathbf{w}_{t-1})$  \n",
    "- $\\mathbf{w}_{t-1}$: tham số trước bước cập nhật  \n",
    "- $|\\mathcal{B}_t|$: kích thước batch  \n",
    "- $\\mathbf{g}_{t,t-1}$: gradient trung bình của mini-batch  \n",
    "\n",
    "Minibatch SGD giúp giảm nhiễu trong từng batch, nhưng vẫn còn nhiễu giữa các batch → cần **leaky average**.\n",
    "\n",
    "\n",
    "\n",
    "### Leaky Average\n",
    "\n",
    "Ý tưởng: lưu trữ gradient tích lũy nhiều bước trước:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t,t-1}\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $\\mathbf{v}_t$: vận tốc (momentum-like)  \n",
    "- $0 \\le \\beta < 1$: hệ số ghi nhớ  \n",
    "- Nếu $\\beta = 0$: chỉ dùng gradient hiện tại  \n",
    "- Nếu $\\beta$ lớn: dùng nhiều thông tin lịch sử, giảm nhiễu nhưng chậm đổi hướng  \n",
    "\n",
    "\n",
    "\n",
    "### Khai triển 2 bước\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_t = \\beta(\\beta \\mathbf{v}_{t-2} + \\mathbf{g}_{t-1,t-2}) + \\mathbf{g}_{t,t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\beta^2 \\mathbf{v}_{t-2} \n",
    "  + \\beta \\mathbf{g}_{t-1,t-2}\n",
    "  + \\mathbf{g}_{t,t-1}\n",
    "$$\n",
    "\n",
    "### Khai triển 3 bước\n",
    "\n",
    "$$\n",
    "= \\beta^3 \\mathbf{v}_{t-3}\n",
    " + \\beta^2 \\mathbf{g}_{t-2,t-3}\n",
    " + \\beta\\, \\mathbf{g}_{t-1,t-2}\n",
    " + \\mathbf{g}_{t,t-1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Khai triển k bước\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_t\n",
    "=\n",
    "\\beta^k \\mathbf{v}_{t-k}\n",
    "+ \\sum_{\\tau=0}^{k-1}\n",
    "  \\beta^\\tau \\mathbf{g}_{t-\\tau,\\,t-\\tau-1}\n",
    "$$\n",
    "\n",
    "### Khai triển t bước\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_t\n",
    "=\n",
    "\\beta^{t} \\mathbf{v}_{0}\n",
    "+ \\sum_{\\tau=0}^{t-1}\n",
    "  \\beta^\\tau \\mathbf{g}_{t-\\tau,\\,t-\\tau-1}\n",
    "$$\n",
    "\n",
    "Với khởi tạo $\\mathbf{v}_0 = 0$:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_t\n",
    "=\n",
    "\\sum_{\\tau=0}^{t-1}\n",
    "\\beta^\\tau \n",
    "\\mathbf{g}_{t-\\tau,\\,t-\\tau-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\mathbf{g}_{t,t-1}\n",
    "+ \\beta \\mathbf{g}_{t-1,t-2}\n",
    "+ \\beta^2 \\mathbf{g}_{t-2,t-3}\n",
    "+ \\cdots\n",
    "$$\n",
    "\n",
    "\n",
    "Với $0 \\le \\beta < 1$:\n",
    "\n",
    "$$\n",
    "1 + \\beta + \\beta^2 + \\cdots\n",
    "=\n",
    "\\frac{1}{1 - \\beta}\n",
    "$$\n",
    "\n",
    "Tổng trọng số lớn hơn 1 ⇒ đây **không phải** Exponential Moving Average (EMA), mà được gọi là: Leaky Average\n",
    "\n",
    "###Leaky Average\n",
    "- Chỉ là tổ hợp tuyến tính giảm dần theo \\(\\beta^\\tau\\)\n",
    "- Tổng trọng số > 1 ⇒ không phải trung bình chuẩn hóa\n",
    "- Giúp làm mượt gradient, giảm nhiễu giữa các batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab902c",
   "metadata": {},
   "source": [
    "\n",
    "## II.2.2 An Ill-conditioned Problem ##\n",
    "\n",
    "$$f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$\n",
    "\n",
    "Hàm $f$ là hàm bậc 2 lồi, có 1 điểm cực tiểu tại $(0, 0)$\n",
    "\n",
    "\n",
    "$$ \\nabla f(x) = (0.2x_1, 4x_2)^T $$\n",
    "\n",
    "\n",
    "\n",
    "Ta thấy gradient theo hướng $x_2$ nhanh hơn gấp 20 lần so với $x_1$.\n",
    "\n",
    "\n",
    "\n",
    "Để minh họa, chúng ta sẽ thử áp dụng Gradient Descent cho hàm này với learning rate = 0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2735f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "\n",
    "eta = 0.4\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "def gd_2d(x1, x2, s1, s2):\n",
    "    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n",
    "\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202d95b",
   "metadata": {},
   "source": [
    "Quát sát hình trên ta thấy, với Gradient Descent với learning rate = 0.4, gradient theo hướng $x_2$ lớn hơn và thay đổi nhanh hơn nhiều so với chiều $x_1$. Do đó, ta chỉ có 2 lựa chọn:\n",
    "- Với learning rate nhỏ, ta đảm bảo $x_2$ không bị phân kì, nhưng $x_1$ sẽ hội tụ rất chậm\n",
    "- Ngược lại, nếu dùng learning rate lớn, ta tiến nhanh theo hướng $x_1$ nhưng lại bị phân kỳ theo hướng $x_2$ ​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b650db",
   "metadata": {},
   "source": [
    "Minh họa learning rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.2\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51968c",
   "metadata": {},
   "source": [
    "Ta thấy Với learning rate 0.2, ta đảm bảo $x_2$ không bị phân kì, nhưng $x_1$ sẽ hội tụ rất chậm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93113ee7",
   "metadata": {},
   "source": [
    "Minh họa learning rate = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.6\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14a46d",
   "metadata": {},
   "source": [
    "Nếu dùng learning rate = 0.6, ta tiến nhanh theo hướng $x_1$ nhưng lại bị phân kỳ theo hướng $x_2$ ​\n",
    "\n",
    "Kết luận: Gradient Descent gặp khó khăn khi giải quyết bài toán  Ill-conditioned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d48ee7",
   "metadata": {},
   "source": [
    "\n",
    "### **I.3 Định nghĩa (Definition)**\n",
    "\n",
    "Trong D2L, momentum được mô tả như việc ta tích luỹ gradient theo thời gian bằng một biến vận tốc (velocity):\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $\\beta$ là hệ số momentum (thường 0.9)\n",
    "- $\\mathbf{v}$ là “quán tính” hướng cập nhật\n",
    "\n",
    "\n",
    "Cập nhật $\\mathbf{x}$:\n",
    "$$\n",
    "\\mathbf{x}_{t} \\;\\leftarrow\\; \\mathbf{x}_{t-1} - \\eta_{t}\\mathbf{v}_{t}.\n",
    "$$\n",
    "\n",
    "**Ý tưởng**: thay vì chỉ dựa vào gradient hiện tại, ta dùng trung bình mượt của gradient giúp cập nhật nhẹ nhàng và ổn định hơn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0414f",
   "metadata": {},
   "source": [
    "### **I.4 Effective Sample Weight**\n",
    "\n",
    "<p style=\"background-color: yellow;color:red\">MISSING</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c4e61",
   "metadata": {},
   "source": [
    "### **I.5 Theoretical Analysis**\n",
    "Giới thiệu\n",
    "\n",
    "Momentum là một kỹ thuật tăng tốc (acceleration technique) rất quan trọng trong tối ưu hoá hiện đại. \n",
    "Ý tưởng cốt lõi của momentum là thay vì chỉ sử dụng gradient hiện tại để cập nhật tham số, ta sẽ tích luỹ \n",
    "một đại lượng gọi là vận tốc (velocity). Đại lượng này mô phỏng quán tính của một vật nặng khi \n",
    "chuyển động trên mặt phẳng hàm mất mát, giúp thuật toán vừa giảm dao động (oscillation) trong những vùng \n",
    "có độ cong lớn, vừa tăng tốc trong những hướng có độ cong nhỏ.\n",
    "\n",
    "Công thức cập nhật momentum (dạng Heavy-ball) được mô tả như sau:\n",
    "$$\n",
    "    \\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t,t-1}, \\\\\n",
    "    \\mathbf{x}_t = \\mathbf{x}_{t-1} - \\eta_t \\mathbf{v}_t.\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "* $\\beta$ là hệ số momentum (thường chọn trong khoảng $0.8$–$0.99$),\n",
    "* $\\mathbf{v}_t$ là vận tốc tích luỹ từ nhiều bước gradient,\n",
    "* $\\eta_t$ là learning rate.\n",
    "\n",
    "**Mục tiêu của phần phân tích lý thuyết:**\n",
    "\n",
    "* Hiểu rõ tại sao momentum giúp tăng tốc quá trình tối ưu hoá.\n",
    "* Hiểu rõ cách momentum giảm dao động trong những vùng có điều kiện kém (ill-conditioned).\n",
    "* Phân tích động học (dynamics) của thuật toán thông qua việc xét dạng hàm bậc hai lồi (quadratic convex function), vốn là trường hợp tiêu chuẩn trong lý thuyết tối ưu.\n",
    "* Chỉ ra điều kiện hội tụ và tốc độ hội tụ.\n",
    "* Đưa ra mô hình 1 chiều (scalar function) để trực quan hoá cơ chế dao động và suy giảm.\n",
    "\n",
    "\n",
    "#### I.5.1 Hàm bậc hai lồi (Quadratic Convex Functions)\n",
    "\n",
    "1. Thiết lập bài toán\n",
    "\n",
    "Xét hàm bậc hai lồi:\n",
    "\\begin{aligned}\n",
    "    f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x} + \\mathbf{b}^\\top \\mathbf{x},\n",
    "\\end{aligned}\n",
    "trong đó $Q$ là ma trận đối xứng xác định dương ($Q \\succ 0$). \n",
    "Gradient của hàm này là:\n",
    "\\begin{aligned}\n",
    "    \\nabla f(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{b}.\n",
    "\\end{aligned}\n",
    "\n",
    "Khi áp dụng momentum:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{v}_t &= \\beta \\mathbf{v}_{t-1} + Q\\mathbf{x}_{t-1} + \\mathbf{b}, \\\\\n",
    "    \\mathbf{x}_t &= \\mathbf{x}_{t-1} - \\eta \\mathbf{v}_t.\n",
    "\\end{aligned}\n",
    "\n",
    "2. Rút gọn hệ phương trình thành phương trình truy hồi bậc hai\n",
    "\n",
    "Ta thay $\\mathbf{v}_t$ vào cập nhật tham số:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{x}_t &= \\mathbf{x}_{t-1} - \n",
    "    \\eta \\left( \\beta\\mathbf{v}_{t-1} + Q\\mathbf{x}_{t-1} + \\mathbf{b} \\right).\n",
    "\\end{aligned}\n",
    "\n",
    "Nhóm lại các hạng tử:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{x}_t \n",
    "    &= (I - \\eta Q)\\mathbf{x}_{t-1} - \\eta\\mathbf{b} - \\eta\\beta\\mathbf{v}_{t-1}.\n",
    "\\end{aligned}\n",
    "\n",
    "Tiếp tục thay $\\mathbf{v}_{t-1}$ từ bước trước:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{v}_{t-1} = \\frac{1}{\\eta} (\\mathbf{x}_{t-1} - \\mathbf{x}_{t-2}).\n",
    "\\end{aligned}\n",
    "\n",
    "Do đó:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{x}_t \n",
    "    &= (I - \\eta Q)\\mathbf{x}_{t-1} \n",
    "    - \\eta\\mathbf{b}\n",
    "    - \\beta (\\mathbf{x}_{t-1} - \\mathbf{x}_{t-2}) \\\\\n",
    "    &= (I - \\eta Q + \\beta I)\\mathbf{x}_{t-1} - \\beta\\mathbf{x}_{t-2} - \\eta\\mathbf{b}.\n",
    "\\end{aligned}\n",
    "\n",
    "Ta thu được phương trình bậc hai:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{x}_t \n",
    "    = A\\mathbf{x}_{t-1}\n",
    "    - \\beta \\mathbf{x}_{t-2}\n",
    "    - \\eta\\mathbf{b},\n",
    "\\end{aligned}\n",
    "trong đó $A = I - \\eta Q + \\beta I$.\n",
    "\n",
    "Đây là bản chất động học của momentum.\n",
    "\n",
    "3. Phân rã theo trị riêng (Eigenvalue Decomposition)\n",
    "\n",
    "Vì $Q$ đối xứng nên ta có thể phân rã:\n",
    "\\begin{aligned}\n",
    "    Q = U\\Lambda U^\\top,\n",
    "\\end{aligned}\n",
    "\n",
    "Trong đó:\n",
    "* $U$ là ma trận trực giao,\n",
    "* $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$.\n",
    "\n",
    "Chuyển hệ sang hệ toạ độ eigen:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{z}_t = U^\\top \\mathbf{x}_t.\n",
    "\\end{aligned}\n",
    "\n",
    "Thay vào phương trình động học, ta có từng chiều độc lập:\n",
    "\\begin{aligned}\n",
    "    z_t^{(i)} = (1 - \\eta\\lambda_i + \\beta) z_{t-1}^{(i)} - \\beta z_{t-2}^{(i)} - \\eta b^{(i)}.\n",
    "\\end{aligned}\n",
    "\n",
    "Các chiều hoạt động hoàn toàn độc lập ⇒ ta chỉ cần phân tích từng chiều riêng lẻ.\n",
    "\n",
    "4. Phương trình đặc trưng\n",
    "\n",
    "Bỏ qua hạng tử hằng để phân tích hội tụ:\n",
    "\\begin{aligned}\n",
    "    z_t^{(i)} = (1 - \\eta\\lambda_i + \\beta) z_{t-1}^{(i)} - \\beta z_{t-2}^{(i)}.\n",
    "\\end{aligned}\n",
    "\n",
    "Đặt nghiệm dạng $z_t^{(i)} = r^t$, ta thu được phương trình đặc trưng:\n",
    "\\begin{aligned}\n",
    "    r^2 - (1 - \\eta\\lambda_i + \\beta) r + \\beta = 0.\n",
    "\\end{aligned}\n",
    "\n",
    "5. Điều kiện hội tụ\n",
    "\n",
    "Hệ hội tụ khi và chỉ khi mọi nghiệm của phương trình đặc trưng đều thoả:\n",
    "\\begin{aligned}\n",
    "    |r| < 1.\n",
    "\\end{aligned}\n",
    "\n",
    "Kết quả quan trọng:  \n",
    "Momentum có thể giảm trị riêng hiệu dụng và làm thuật toán hội tụ nhanh hơn đáng kể.\n",
    "\n",
    "6. Tốc độ hội tụ\n",
    "\n",
    "Nếu chọn tối ưu:\n",
    "\\begin{aligned}\n",
    "    \\eta = \\frac{1}{L}, \\qquad\n",
    "    \\beta = \\frac{\\sqrt{L}-\\sqrt{\\mu}}{\\sqrt{L}+\\sqrt{\\mu}},\n",
    "\\end{aligned}\n",
    "ta thu được:\n",
    "\\begin{aligned}\n",
    "    f(\\mathbf{x}_t) - f(\\mathbf{x}^\\*)\n",
    "    = O\\left(\\frac{1}{t^2}\\right).\n",
    "\\end{aligned}\n",
    "\n",
    "Đây là tốc độ hội tụ nhanh hơn Gradient Descent:\n",
    "\\begin{aligned}\n",
    "    O\\left( \\frac{1}{t} \\right).\n",
    "\\end{aligned}\n",
    "\n",
    "####I.5.2 Hàm một biến (Scalar Functions)\n",
    "\n",
    "Trong mục này, ta xét trường hợp đơn giản nhất:\n",
    "\\begin{aligned}\n",
    "    f(x) = \\frac{1}{2}\\lambda x^2,\n",
    "\\end{aligned}\n",
    "với $\\lambda > 0$.\n",
    "\n",
    "1. Gradient và momentum\n",
    "\n",
    "Gradient:\n",
    "\\begin{aligned}\n",
    "    f'(x) = \\lambda x.\n",
    "\\end{aligned}\n",
    "\n",
    "Momentum:\n",
    "\\begin{aligned}\n",
    "    v_t &= \\beta v_{t-1} + \\lambda x_{t-1}, \\\\\n",
    "    x_t &= x_{t-1} - \\eta v_t.\n",
    "\\end{aligned}\n",
    "\n",
    "2. Viết lại thành phương trình bậc hai\n",
    "\n",
    "Thay $v_t$:\n",
    "\\begin{aligned}\n",
    "    x_t \n",
    "    &= x_{t-1} - \\eta(\\beta v_{t-1} + \\lambda x_{t-1}) \\\\\n",
    "    &= (1 - \\eta\\lambda)x_{t-1} - \\eta\\beta v_{t-1}.\n",
    "\\end{aligned}\n",
    "\n",
    "Từ cập nhật của bước trước:\n",
    "\\begin{aligned}\n",
    "    x_{t-1} = x_{t-2} - \\eta v_{t-1}\n",
    "    \\quad\\Rightarrow\\quad\n",
    "    v_{t-1} = \\frac{x_{t-2} - x_{t-1}}{\\eta}.\n",
    "\\end{aligned}\n",
    "\n",
    "Thay vào:\n",
    "\\begin{aligned}\n",
    "    x_t \n",
    "    &= (1 - \\eta\\lambda)x_{t-1} \n",
    "    - \\beta(x_{t-2} - x_{t-1}) \\\\\n",
    "    &= (1 - \\eta\\lambda + \\beta)x_{t-1} - \\beta x_{t-2}.\n",
    "\\end{aligned}\n",
    "\n",
    "3. Phương trình đặc trưng\n",
    "\n",
    "Xét nghiệm dạng $x_t = r^t$:\n",
    "\\begin{aligned}\n",
    "    r^2 - (1 - \\eta\\lambda + \\beta)r + \\beta = 0.\n",
    "\\end{aligned}\n",
    "\n",
    "4. Hành vi động học (Dynamics)\n",
    "\n",
    "* Nếu nghiệm phức: thuật toán dao động giảm dần.\n",
    "* Nếu nghiệm thực: thuật toán giảm đơn điệu.\n",
    "\n",
    "Điều kiện:\n",
    "\\begin{aligned}\n",
    "    |r| < 1.\n",
    "\\end{aligned}\n",
    "\n",
    "Kết luận:\n",
    "* $\\lambda$ lớn → gradient lớn → dễ dao động → momentum giảm dao động.\n",
    "* $\\lambda$ nhỏ → gradient nhỏ → GD chậm → momentum tăng tốc.\n",
    "\n",
    "**Chứng minh hội tụ**\n",
    "\n",
    "(1) Momentum hội tụ khi nghiệm của phương trình đặc trưng thoả:\n",
    "\\begin{aligned}\n",
    "    |r| < 1.\n",
    "\\end{aligned}\n",
    "\n",
    "(2) Từ phân tích trị riêng, điều này đúng khi:\n",
    "\\begin{aligned}\n",
    "    0 < \\eta < \\frac{2}{L}(1+\\beta), \\\\\n",
    "    0 \\le \\beta < 1.\n",
    "\\end{aligned}\n",
    "\n",
    "(3) Khi chọn tối ưu (Nesterov/Polyak):\n",
    "\\begin{aligned}\n",
    "    \\eta = \\frac{1}{L}, \\qquad \n",
    "    \\beta = \\frac{\\sqrt{L}-\\sqrt{\\mu}}{\\sqrt{L}+\\sqrt{\\mu}},\n",
    "\\end{aligned}\n",
    "ta chứng minh được rằng:\n",
    "$$\n",
    "f(\\mathbf{x}_t) - f(\\mathbf{x}^\\*) = O\\left(\\frac{1}{t^2}\\right)\n",
    "$$\n",
    "\n",
    "Điều này nhanh hơn rõ rệt so với Gradient Descent:\n",
    "\\begin{aligned}\n",
    "    O\\left(\\frac{1}{t}\\right).\n",
    "\\end{aligned}\n",
    "\n",
    "Qua đó ta kết luận rằng:\n",
    "* Momentum tạo ra hệ động học bậc hai,\n",
    "* Giảm dao động theo chiều có độ cong lớn,\n",
    "* Tăng tốc theo chiều có độ cong nhỏ,\n",
    "* Và đạt tốc độ hội tụ tối ưu cho lớp hàm lồi trơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aaf71e",
   "metadata": {},
   "source": [
    "### **II Ứng dụng**\n",
    "\n",
    "### II.1 Ứng dụng của Momentum\n",
    "\n",
    "#### II.1.1. Huấn luyện deep neural networks\n",
    "\n",
    "Trong các mạng lớn như CNN, RNN, Transformer, bề mặt loss rất phức tạp → Momentum giúp:\n",
    "\n",
    "- Giảm dao động gradient theo các chiều dốc.\n",
    "- Tăng tốc theo các chiều phẳng (flat/sloppy directions).\n",
    "- Hội tụ nhanh hơn nhiều so với SGD thông thường.\n",
    "\n",
    "\n",
    "#### II.1.2. Giảm vấn đề mạng vanishing gradient\n",
    "\n",
    "Ở những vùng gradient rất nhỏ, Momentum có thể tích lũy nhiều bước để vẫn tạo ra cập nhật đủ lớn để thoát vùng “chết”.\n",
    "\n",
    "#### II.1.3. Tối ưu bài toán ML cổ điển\n",
    "\n",
    "- Logistic regression\n",
    "- Linear regression\n",
    "- Matrix factorization\n",
    "- Collaborative filtering\n",
    "- PCA optimization (stochastic gradient)\n",
    "\n",
    "Momentum giúp các thuật toán này tiến nhanh hơn và ít dao động.\n",
    "\n",
    "#### II.1.4. Tối ưu trong Robotics / Control / Physics Simulation\n",
    "\n",
    "Do bản chất mô phỏng từ vật lý (momentum), thuật toán hoạt động hiệu quả trong hệ động lực.\n",
    "\n",
    "### **II.2. Cách Momentum giải quyết một số bài toán Khoa học Máy tính**\n",
    "\n",
    "#### II.2.1. **Bài toán rãnh hẹp (ravine problem)**\n",
    "\n",
    "Như ví dụ $(1.1)$: Gradient descent thường dao động mạnh ở chiều , tiến chậm ở chiều .\n",
    "Momentum giúp lấy hướng trung bình → giảm zigzag.\n",
    "\n",
    "#### II.2.2. **Bài toán logistic regression với dữ liệu không chuẩn hóa**\n",
    "\n",
    "Khi một số feature có scale lớn – nhỏ khác nhau, gradient descent rất “nảy”.\n",
    "Momentum giúp cân bằng việc cập nhật và giảm jitter.\n",
    "\n",
    "#### II.2.3. **Bài toán tối ưu mạng sâu (deep network loss landscape)**\n",
    "\n",
    "Landscape của deep learning thường có:\n",
    "\n",
    "- Saddle points\n",
    "- Flat region\n",
    "- Sharp minima\n",
    "\n",
    "SGD thường đứng yên ở flat region → Momentum tích lũy gradient và “đẩy qua” vùng này.\n",
    "\n",
    "#### II.2.4. **Bài toán SGD với batch nhỏ**\n",
    "\n",
    "Batch nhỏ → gradient nhiễu.\n",
    "Momentum giúp làm mượt (smoothing) gradient theo thời gian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b84895",
   "metadata": {},
   "source": [
    "<!-- ############ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9622b6d",
   "metadata": {},
   "source": [
    "<!-- ######################################################################################################## -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7dda9",
   "metadata": {},
   "source": [
    "## III. Bài tập\n",
    "---\n",
    "\n",
    "> Các bài tập trong nội dung này được tham khảo trong phần 12.6.5. Exercises ở trang D2L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16abfd8",
   "metadata": {},
   "source": [
    "### III.1. Bài 1\n",
    "Use other combinations of momentum hyperparameters and learning rates and observe and analyze the different experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4116bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SYSTEMATIC EXPERIMENTS FOR CLEAR CONCLUSIONS\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def momentum_2d(x1, x2, v1, v2):\n",
    "    v1 = beta * v1 + 0.2 * x1\n",
    "    v2 = beta * v2 + 4 * x2\n",
    "    return x1 - eta * v1, x2 - eta * v2, v1, v2\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 1: Effect of Momentum (Fixed LR)\n",
    "# Kết luận: Beta tăng → mượt hơn, ESS tăng, hội tụ tốt hơn\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: Effect of Momentum (Fixed eta=0.6)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Mục đích: Quan sát momentum từ 0 → 0.95, ESS từ 1 → 20\")\n",
    "print()\n",
    "\n",
    "eta = 0.6\n",
    "beta_experiments = [\n",
    "    (0.0,  \"No momentum (ESS=1)\"),\n",
    "    (0.3,  \"Low momentum (ESS≈1.4)\"),\n",
    "    (0.5,  \"Medium momentum (ESS=2)\"),\n",
    "    (0.7,  \"Medium-high (ESS≈3.3)\"),\n",
    "    (0.9,  \"High/Standard (ESS=10)\"),\n",
    "    (0.95, \"Very high (ESS=20)\"),\n",
    "]\n",
    "\n",
    "for beta, description in beta_experiments:\n",
    "    print(f\"\\neta={eta}, beta={beta} - {description}\")\n",
    "    print(f\"  Effective step size: {eta/(1-beta) if beta < 1 else 'inf':.2f}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\n",
    "    plt.title(f'eta={eta}, beta={beta} ({description})', fontsize=12, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nKẾT LUẬN EXPERIMENT 1:\")\n",
    "print(\"- Beta càng cao → đường đi càng mượt, ít dao động\")\n",
    "print(\"- Beta=0.0 dao động nhiều, đặc biệt ở hướng x2 (steep)\")\n",
    "print(\"- Beta > 0.5 bắt đầu phân kì, bước nhảy lớn đã hất văng nghiệm ra xa điểm cực tiểu\")\n",
    "print(\"=> Khi tăng Beta lên cao, BẮT BUỘC phải giảm Eta xuống\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 2: Effect of Learning Rate (Fixed Momentum)\n",
    "# Kết luận: Với beta=0.9, cần điều chỉnh eta phù hợp\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2: Effect of Learning Rate (Fixed beta=0.9)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Mục đích: Với momentum cao, thử các learning rate khác nhau\")\n",
    "print()\n",
    "\n",
    "beta = 0.9\n",
    "eta_experiments = [\n",
    "    (0.01, \"Very low LR\"),\n",
    "    (0.05, \"Very low LR\"),\n",
    "    (0.1, \"Very low LR\"),\n",
    "    (0.2, \"Low LR\"),\n",
    "    (0.4, \"Medium LR\"),\n",
    "    (0.6, \"High LR\"),\n",
    "    (0.8, \"Very high LR\"),\n",
    "]\n",
    "\n",
    "for eta, description in eta_experiments:\n",
    "    print(f\"\\neta={eta}, beta={beta} - {description}\")\n",
    "    print(f\"  Effective step size: {eta/(1-beta):.2f}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\n",
    "    plt.title(f'eta={eta}, beta={beta} ({description})', fontsize=12, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nKẾT LUẬN EXPERIMENT 2:\")\n",
    "print(\"Khi Momentum cao beta=0.9\")\n",
    "print(\"- nếu eta=0.01 quá nhỏ, tốc độ hội tụ cực chậm\")\n",
    "print(\"- nếu eta=0.05, hội tụ tốt, giao động ổn định\")\n",
    "print(\" - khi eta > 0.05, trạng thái giao động bắt đầu có dấu hiệu giao động mạnh\")\n",
    "print(\"việc tăng Learning Rate dù rất ít(từ 0.1 lên 0.2) cũng làm tăng Effective Step Size lên gấp đôi, chuyển trạng thái từ 'dao động ổn định' sang 'dao động mạnh/kém hiệu quả'.\")\n",
    "print(\"=> Khi tăng Beta cao 0.9, nếu eta cao thì sẽ khuếch đại dao động đáng kể, dễ gây ra giao động mạnh dẫn đến phân kỳ\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 3: Best vs Worst Combinations\n",
    "# Kết luận: Đưa ra best practices\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 4: Best vs Worst Combinations\")\n",
    "print(\"=\"*70)\n",
    "print(\"Mục đích: Thấy rõ cấu hình tốt và xấu\")\n",
    "print()\n",
    "\n",
    "comparison_experiments = [\n",
    "    (0.6, 0.0,  \"WORST: High LR + No momentum\"),\n",
    "    (0.8, 0.9,  \"BAD: Too aggressive\"),\n",
    "    (0.1, 0.3,  \"BAD: Too conservative\"),\n",
    "    (0.4, 0.9,  \"BAD: Too aggressive\"),\n",
    "    (0.6, 0.5,  \"GOOD: Medium combination\"),\n",
    "    (0.05, 0.9,  \"BEST: Safe and smooth\"),\n",
    "]\n",
    "\n",
    "for eta, beta, label in comparison_experiments:\n",
    "    print(f\"\\n{label}: eta={eta}, beta={beta}\")\n",
    "    print(f\"  Effective step: {eta/(1-beta) if beta < 1 else 'inf':.2f}, ESS: {1/(1-beta):.1f}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\n",
    "    plt.title(f'{label}\\neta={eta}, beta={beta}', fontsize=12, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nKẾT LUẬN EXPERIMENT 3:\")\n",
    "print(\"WORST practices:\")\n",
    "print(\"- Learning Rate cao + Không có Momentum: Dẫn đến dao động mạnh mà không hội tụ được\")\n",
    "print(\"- eta và beta đều cao → phân kỳ\")\n",
    "print(\"\\nBEST practices:\")\n",
    "print(\"- beta cao (0.9) +  eta nhỏ (< 0.6)\")\n",
    "print(\"- Trade-off: eta↑ → beta↓ để giữ effective step hợp lý\")\n",
    "print(\"- Với ill-conditioned problems: beta=0.9, eta <= 0.5\")\n",
    "\n",
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KẾT LUẬN TỔNG HỢP\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. MOMENTUM GIÚP:\n",
    "   - Làm mịn gradient noise (tích lũy ESS gradients)\n",
    "   - Tăng tốc ở hướng consistent (x1 - flat direction)\n",
    "   - Giảm dao động ở hướng steep (x2 - steep direction)\n",
    "\n",
    "2. BEST PRACTICES:\n",
    "   - Bắt đầu: η=0.4, β=0.9 (effective step ≈ 4)\n",
    "   - An toàn: η=0.2, β=0.9 (effective step = 2)\n",
    "   - Nhanh hơn: η=0.6, β=0.9 (effective step = 6)\n",
    "   - Tránh: β=0 với η cao, hoặc β và η đều cao\n",
    "\n",
    "3. SO SÁNH VỚI SGD:\n",
    "   - SGD (β=0): cần η rất nhỏ để ổn định\n",
    "   - Momentum (β=0.9): có thể dùng η lớn hơn vì smooth hơn\n",
    "   - Momentum hội tụ nhanh hơn VÀ ổn định hơn\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6b2db",
   "metadata": {},
   "source": [
    "### III.2 Bài 2\n",
    "Try out gradient descent and momentum for a quadratic problem where you have multiple eigenvalues, i.e.,\n",
    "\n",
    "\\begin{aligned}\n",
    "f(x)=\\frac{1}{2}\\sum_{i}\\lambda_{i}x_{i}^2, \\qquad \\lambda_{i}=2^{-i}\n",
    "\\end{aligned}\n",
    "\n",
    "Plot how the values of $x$ decrease for the initialization $x_i=1$.\n",
    "\n",
    "**\n",
    "\n",
    "Ta có:\n",
    "\n",
    "\\begin{aligned}\n",
    "f(x)\n",
    "&=\\frac{1}{2}\\sum_{i}\\lambda_{i}x_{i}^2, \\qquad \\lambda_{i}=2^{-i} \\\\\n",
    "&=\\frac{1}{2}\\sum_{i} 2^{-i}x_i^2\n",
    "\\end{aligned}\n",
    "\n",
    "Gọi $\\mathbf{x} = [x_1, x_2,...,x_i]^\\top$\n",
    "\n",
    "Với **Gradient Descent**:\n",
    "\n",
    "- Khởi tạo:\n",
    "  \\begin{aligned}\n",
    "  \\mathbf{x}_{0} &= [1,1,...,1]^\\top \\\\\n",
    "  \\eta &= 0.1\n",
    "  \\end{aligned}\n",
    "\n",
    "- Tính $\\nabla f(x)$:\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\nabla f(x) \n",
    "  &= [\n",
    "    \\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2}, ..., \\frac{\\partial f(x)}{\\partial x_i}\n",
    "  ]^\\top \\\\\n",
    "  &= [\n",
    "    \\frac{\\partial (\\frac{1}{2}2^{-1}x_1^2)}{\\partial x_1},\n",
    "    \\frac{\\partial (\\frac{1}{2}2^{-2}x_2^2)}{\\partial x_2},...,\n",
    "    \\frac{\\partial (\\frac{1}{2}2^{-i}x_i^2)}{\\partial x_i},\n",
    "  ]^\\top \\\\\n",
    "  &= [2^{-1}x_1, 2^{-2}x_2,..., 2^{-i}x_i]^\\top\n",
    "  \\end{aligned}\n",
    "\n",
    "- Vòng lặp 1:\n",
    "  - Bước 1: $\\nabla f(x_{0}) = [2^{-1}, 2^{-2},..., 2^{-i}]$\n",
    "  - Bước 2:\n",
    "    \\begin{aligned}\n",
    "    \\mathbf{x}_1 &= \\mathbf{x}_0 - \\eta \\nabla f(x_0) \\\\\n",
    "    &= [1,1,...,1]^\\top - 0.1[2^{-1}x_1, 2^{-2}x_2,..., 2^{-i}x_i]^\\top \\\\\n",
    "    &= [0.95, 0.975,...,1 - 0.1 \\cdot 2^{-i}]\n",
    "    \\end{aligned}\n",
    "\n",
    "**Nhận xét**: điểm tối ưu sẽ là vector $\\mathbf{x}^*=[0, 0, ..., 0]^\\top$\n",
    "\n",
    "Phương pháp Gradient Descent sẽ làm cho các vị trí $x_i$ với $i$ lớn sẽ về điểm $\\mathcal O$ rất lâu.<br>\n",
    "Nếu $i \\rightarrow \\infty$, thậm chí còn không di chuyển.\n",
    "\n",
    "**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ccc98",
   "metadata": {},
   "source": [
    "Với **Momentum**:\n",
    "\n",
    "- Khởi tạo:\n",
    "  \\begin{aligned}\n",
    "  \\beta &= 0.5 \\\\\n",
    "  \\eta &= 0.1 \\\\\n",
    "  \\mathbf{v}_0 &= [0,0,...,0]^\\top \\\\\n",
    "  \\mathbf{x}_0 &= [1,1,...,1]^\\top\n",
    "  \\end{aligned}\n",
    "\n",
    "- Tính $\\nabla f(x) = [2^{-1}x_1, 2^{-2}x_2,..., 2^{-i}x_i]^\\top$:\n",
    "\n",
    "- Vòng lặp 1:\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\nabla f(x_{0}) &= [2^{-1}, 2^{-2},..., 2^{-i}]^\\top \\\\\n",
    "  \\\\\n",
    "  \\mathbf{v}_1 \n",
    "  &= \\beta \\mathbf{v}_0 + \\nabla f(x_0) \\\\\n",
    "  &= 0.5[0,0,...,0]^\\top + [2^{-1}, 2^{-2},..., 2^{-i}]^\\top \\\\\n",
    "  &= [2^{-1}, 2^{-2},..., 2^{-i}]^\\top \\\\\n",
    "  &= [\\lambda_{1},\\lambda_{2},...,\\lambda_{i}]^\\top \\\\\n",
    "  \\\\\n",
    "  \\mathbf{x}_1\n",
    "  &= \\mathbf{x}_0 - \\eta \\mathbf{v}_1 \\\\\n",
    "  &= [1,1,...,1]^\\top - 0.1[2^{-1}, 2^{-2},..., 2^{-i}]^\\top \\\\\n",
    "  &= [0.95, 0.975,..., 1 - 0.1 \\cdot 2^{-i}]^\\top \\\\\n",
    "  &= [1-0.1\\lambda_{1}, 1-0.1\\lambda_{2}, ..., 1-0.1\\lambda_{i}]^\\top\n",
    "  \\end{aligned}\n",
    "\n",
    "- Vòng lặp 2:\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\nabla f(x_{1}) \n",
    "  &= [\\lambda_{1}(1-0.1\\lambda_{1}), \\lambda_{2}(1-0.1\\lambda_{2}), ..., \\lambda_{i}(1-0.1\\lambda_{i})]^\\top \\\\\n",
    "  &= [\\lambda_{1} - 0.1\\lambda_{1}^2, \\lambda_{2} - 0.1\\lambda_{2}^2, ..., \\lambda_{i} - 0.1\\lambda_{i}^2]^\\top\n",
    "  \\\\\n",
    "  \\mathbf{v}_2\n",
    "  &= \\beta \\mathbf{v}_1 + \\nabla f(x_1) \\\\\n",
    "  &= 0.5[\\lambda_{1},\\lambda_{2},...,\\lambda_{i}]^\\top + [\\lambda_{1} - 0.1\\lambda_{1}^2, \\lambda_{2} - 0.1\\lambda_{2}^2, ..., \\lambda_{i} - 0.1\\lambda_{i}^2]^\\top \\\\\n",
    "  &= [1.5\\lambda_{1}-0.1\\lambda_{1}^2, 1.5\\lambda_{2}-0.1\\lambda_{2}^2,...,1.5\\lambda_{i}-0.1\\lambda_{i}^2]^\\top \\\\\n",
    "  \\\\\n",
    "  \\mathbf{x}_2\n",
    "  &= \\mathbf{x}_1 - \\eta \\mathbf{v}_2 \\\\\n",
    "  &= [1-0.1\\lambda_{1}, 1-0.1\\lambda_{2}, ..., 1-0.1\\lambda_{i}]^\\top - 0.1[1.5\\lambda_{1}-0.1\\lambda_{1}^2, 1.5\\lambda_{2}-0.1\\lambda_{2}^2,...,1.5\\lambda_{i}-0.1\\lambda_{i}^2]^\\top \\\\\n",
    "  &= [1-0.25\\lambda_{1}-0.01\\lambda_{1}^2, 1-0.25\\lambda_{2}-0.01\\lambda_{2}^2, ..., 1-0.25\\lambda_{i}-0.01\\lambda_{i}^2]^\\top\n",
    "  \\end{aligned}\n",
    "\n",
    "**Nhận xét:**\n",
    "\n",
    "Khi $i \\rightarrow \\infty$, lúc này tốc độ tiền về điểm tối ưu của biến $x_i$ cải thiện hơn so với phương pháp Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15e1b8",
   "metadata": {},
   "source": [
    "<!-- ############################################################################################### -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61d07f",
   "metadata": {},
   "source": [
    "### III.3 Bài 3\n",
    "Derive minimum value and minimizer for $h(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^\\top\\mathbf{Q}\\mathbf{x} + \\mathbf{x}^\\top\\mathbf{c} + b$\n",
    "\n",
    "**\n",
    "\n",
    "\\begin{aligned}\n",
    "h(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^\\top\\mathbf{Q}\\mathbf{x} + \\mathbf{x}^\\top\\mathbf{c} + b,\n",
    "\\qquad Q \\in \\mathbb{R}^{n \\times n};\n",
    "\\mathbf{x}, \\mathbf{c} \\in \\mathbb{R}^n;\n",
    "b \\in \\mathbb{R}\n",
    "\\tag{4.2.1}\n",
    "\\end{aligned}\n",
    "\n",
    "Mục tiêu: tìm điểm tối ưu và giải bằng Momentum đúng theo d2l.ai (dùng công thức tích lũy momentum dạng EMA và cập nhật tham số có learning rate $\\eta$).\n",
    "\n",
    "Trong đó, \n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}^\\top &= [x_1, x_2, ..., x_n]^\\top \n",
    "\\Rightarrow \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\mathbf{c} &= \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\n",
    "\\\\\n",
    "\\mathbf{Q} &= \\begin{bmatrix}\n",
    "  Q_{11} & Q{12} & \\cdots & Q_{1n} \\\\\n",
    "  Q_{21} & Q{22} & \\cdots & Q_{2n} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  Q_{n1} & Q{n2} & \\cdots & Q_{nn}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073f806",
   "metadata": {},
   "source": [
    "#### 1. Gradient của hàm\n",
    "##### Xét $\\mathbf{x}^\\top\\mathbf{Q}\\mathbf{x}$\n",
    "\n",
    "Theo định nghĩa ma trận $(\\mathbf{Q}\\mathbf{x})_i = \\sum_{j=1}^n \\mathbf{Q}_{ij}\\mathbf{x}_j$,\n",
    "nghĩa là: $\\mathbf{Q}\\mathbf{x} = \\begin{bmatrix} \\sum_{j=1}^n \\mathbf{Q}_{1j}\\mathbf{x}_j \\\\ \\sum_{j=1}^n \\mathbf{Q}_{2j}\\mathbf{x}_j \\\\ \\vdots \\\\ \\sum_{j=1}^n \\mathbf{Q}_{nj}\\mathbf{x}_j \\end{bmatrix}$\n",
    "\n",
    "$$\\Rightarrow \\mathbf{x}^\\top Q\\mathbf{x} = \\sum_{i=1}^n \\sum_{j=1}^n \\mathbf{x}_i Q_{ij} \\mathbf{x}_j, \\qquad \\mathbf{x}^\\top \\mathbf{c}=\\sum_{i=1}^n \\mathbf{x}_i c_i$$\n",
    "\n",
    "Lấy đạo hàm riêng theo biến k. Ta quan sát hai tường hợp khi $\\mathbf{x}_k$ xuất hiện trong hai trường hợp:\n",
    "\n",
    "-  Nếu $(i = k)$ thì hạng tử có dạng $(\\mathbf{x}_k Q_{kj} \\mathbf{x}_j)$ → đạo hàm theo $(\\mathbf{x}_k)$ cho $(Q_{kj} \\mathbf{x}_j)$\n",
    "\n",
    "-  Nếu $(j=k)$, hạng tử có dạng $(\\mathbf{x}_i Q_{ik} \\mathbf{x}_k)$ → đạo hàm theo $(\\mathbf{x}_k)$ cho $(\\mathbf{x}_i Q_{ik})$.\n",
    "\n",
    "Do đó\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{x}_k}\\left(\\tfrac12\\sum_{i,j} \\mathbf{x}_i Q_{ij} \\mathbf{x}_j\\right)=\\tfrac12\\sum_{j} Q_{kj} \\mathbf{x}_j + \\tfrac12\\sum_{i} \\mathbf{x}_i Q_{ik}$$\n",
    "\n",
    "Cộng phần từ $(\\mathbf{x}^\\top\\mathbf{c})$ và hằng số, ta được\n",
    "$$\n",
    "\\frac{\\partial h}{\\partial \\mathbf{x}_k}=\\tfrac12\\sum_j Q_{kj}\\mathbf{x}_j + \\tfrac12\\sum_i \\mathbf{x}_i Q_{ik} + c_k\n",
    "$$\n",
    "\n",
    "Viết gọn bằng ma trận (chú ý $(Q^\\top_{kj}=Q_{jk})$):\n",
    "$$\\nabla h(\\mathbf{x})=\\tfrac12(Q+Q^\\top)\\mathbf{x}+\\mathbf{c}$$\n",
    "Khi (Q) đối xứng (thường là giả thiết cho quadratic form), $(Q=Q^\\top)$ và\n",
    "\n",
    "$$\n",
    "\\boxed{\\nabla h(\\mathbf{x})=Q\\mathbf{x}+\\mathbf{c}}\n",
    "\\tag{4.2.2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3109cf",
   "metadata": {},
   "source": [
    "#### 2. Nghiệm đóng\n",
    "\n",
    "Điều kiện điểm tới hạn:\n",
    "$$\n",
    "\\nabla h(\\mathbf{x}^*)=0\\quad\n",
    "\\iff \\quad Q\\mathbf{x}^* + \\mathbf{c}=0\n",
    "\\iff \\quad Q\\mathbf{x}^* = \\mathbf{-c}\n",
    "$$\n",
    "\n",
    "Nếu $(\\mathbf{Q})$ khả nghịch (invertible - $\\mathbf{Q}$ * $\\mathbf{Q}^{-1}$ = I), nhân hai vế với $(\\mathbf{Q}^{-1})$ ta có:\n",
    "$$\n",
    "\\mathbf Q^{-1} \\mathbf{Q} \\mathbf{x}^* = \\mathbf{-c}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathbf{x}^*=-\\mathbf{Q}^{-1}\\mathbf{c}}\n",
    "\\tag{4.2.3}\n",
    "$$\n",
    "\n",
    "Giá trị tối thiểu:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}^*) =\\tfrac12(\\mathbf{x}^*)^\\top \\mathbf{Q}\\mathbf{x}^* + (\\mathbf{x}^*)^\\top \\mathbf{c} + b\n",
    "$$\n",
    "\n",
    "Thay $\\mathbf{x}^*$ = $\\mathbf{-Q}^{-1}\\mathbf{c}$, ta có:\n",
    "\n",
    "\\begin{aligned}\n",
    "h(\\mathbf{-Q}^{-1}\\mathbf{c})\n",
    "  &=\\tfrac12(\\mathbf{-Q}^{-1}\\mathbf{c})^\\top \\mathbf{Q}(\\mathbf{-Q}^{-1}\\mathbf{c}) + (\\mathbf{-Q}^{-1}\\mathbf{c})^\\top \\mathbf{c} + b \\\\\n",
    "  &=\\tfrac12\\mathbf{c}^\\top\\mathbf{Q}^{-1}\\mathbf{c} - \\mathbf{c}^\\top\\mathbf{Q}^{-1} \\mathbf{c} + b \\\\\n",
    "  &= b - \\tfrac12\\mathbf{c}^\\top \\mathbf{Q}^{-1}\\mathbf{c}\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df47ad1",
   "metadata": {},
   "source": [
    "#### 3. Công thức Momentum\n",
    "\n",
    "Ta định nghĩa gradient tại bước t là:\n",
    "$$\\mathbf{g}_t = \\nabla h(\\mathbf{x}_t)$$\n",
    "\n",
    "Momentum (EMA style) được viết bằng công thức truy hồi:\n",
    "$$\\mathbf{v}_t = \\beta\\mathbf{v}_{t-1} + \\mathbf{g}_{t,t-1},\\qquad \\mathbf{v}_0=0$$\n",
    "\n",
    "Và sau khi khai triển ta nhận được dạng tương đương (công thức 12.6.3 của d2l):\n",
    "$$\\mathbf{v}t = \\sum_{\\tau=0}^{t-1} \\beta^{\\tau}\\mathbf{g}_{t-\\tau,t-\\tau-1}$$\n",
    "\n",
    "Cuối cùng, cập nhật tham số sử dụng learning rate ($\\eta>0$):\n",
    "$$\\mathbf{x}_t = \\mathbf{x}_{t-1} - \\eta\\mathbf{v}_t$$\n",
    "\n",
    "Lưu ý: ở dạng d2l, $(\\eta)$ xuất hiện khi cập nhật $(\\mathbf{x}_t)$, còn $(\\mathbf{v}_t)$ mô tả sự tích lũy của gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ebfcee",
   "metadata": {},
   "source": [
    "#### 4. Áp dụng Momentum cho bài toán Quadratic\n",
    "\n",
    "- Thiết lập notation:\n",
    "  \n",
    "  * Ta đã có: $\\nabla h(\\mathbf{x}) = Q\\mathbf{x}+\\mathbf{c}$\n",
    "  * Do đó ở bước $t$, ta có: $\\mathbf{g}_t = Q\\mathbf{x}_t + \\mathbf{c}$\n",
    "\n",
    "- Chọn tham số:\n",
    "\n",
    "  *\thệ số momentum $(\\beta\\in[0,1))$ (thường là 0.9)\n",
    "  *\tlearning rate $(\\eta>0)$ chọn sao cho ổn định\n",
    "  *\tkhởi tạo $(\\mathbf{x}_0)$ (tùy ý), $(\\mathbf{v}_0=0)$.\n",
    "\n",
    "- Lặp cho t = 0,1,2,...:\n",
    "\n",
    "  1. Tính gradient: $(\\mathbf{g}_t = Q\\mathbf{x}_t + \\mathbf{c})$\n",
    "  2. Tích lũy momentum: $(\\mathbf{v}_{t+1} = \\beta\\mathbf{v}_t + \\mathbf{g}_t)$\n",
    "  3. Cập nhật tham số: $(\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta_t\\mathbf{v}_{t+1})$\n",
    "\n",
    "- Tại sao thuật toán tìm về nghiệm đóng:\n",
    "\n",
    "  * Điểm dừng (nếu hội tụ) là $(\\mathbf{x}_{t+1} = \\mathbf{x}_t = \\mathbf{x}^*)$, và theo đó $(\\mathbf{v}_{t+1}=\\mathbf{v}_t = \\mathbf{v}^*)$ và $(\\mathbf{g}_t=\\mathbf{g}^* = Q \\mathbf{x}^* + \\mathbf{c})$.\n",
    "\n",
    "  * Từ quy luật tích lũy:\n",
    "  $$\\mathbf{v}^* = \\beta\\mathbf{v}^* + \\mathbf{g}^* \\quad\\Rightarrow\\quad (1-\\beta)\\mathbf{v}^* = \\mathbf{g}^*$$\n",
    "\n",
    "  * Từ quy luật cập nhật ($\\mathbf{x}$) và $(\\mathbf{x}_{t+1} = \\mathbf{x}_t = \\mathbf{x}^*)$,  $(\\mathbf{v}_{t+1}=\\mathbf{v}_t = \\mathbf{v}^*)$\n",
    "  ta có:\n",
    "  \\begin{aligned}\n",
    "  &\\mathbf{x}_{t} = \\mathbf{x}_t - \\eta_t\\mathbf{v}_{t}^* \\quad \\\\\n",
    "  &\\iff -\\eta\\mathbf{v}^* = 0 \\quad \\Rightarrow \\quad \\mathbf{v}^* = 0\n",
    "  \\end{aligned}\n",
    "\n",
    "  * Kết hợp hai phương trình trên cho ta $(\\mathbf{g}^*=0)$. Do đó:\n",
    "  $$Q\\mathbf{x}^* + \\mathbf{c} = 0 \\quad \\Rightarrow \\quad \\mathbf{x}^* = -Q^{-1} \\mathbf{c}$$\n",
    "\n",
    "(nếu (Q) khả nghịch).\n",
    "\n",
    "Vì vậy nghiệm hội tụ của thuật toán (nếu hội tụ về điểm cố định) là nghiệm đóng đã biết."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096ce17",
   "metadata": {},
   "source": [
    "#### 5. Giá trị nhỏ nhất\n",
    "\n",
    "Khi $(\\mathbf{x}^* = -Q^{-1} \\mathbf{c})$\n",
    "\n",
    "Giá trị nhỏ nhất là:\n",
    "$h(\\mathbf{x}^*) = b - \\tfrac12\\mathbf{c}^\\top Q^{-1}\\mathbf{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec24a7",
   "metadata": {},
   "source": [
    "#### 6. Tóm tắt\n",
    "\n",
    "- Gradient:\n",
    "$$(\\nabla h(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{c})$$\n",
    "\n",
    "- Momentum (d2l):\n",
    "$$\\mathbf{v}_t= \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t-1}, \\quad \\mathbf{x}_t=\\mathbf{x}_{t-1} - \\eta \\mathbf{v}_t$$\n",
    "\n",
    "- Nghiệm đóng (nếu Q khả nghịch):\n",
    "$$\\mathbf{x}^* = -Q^{-1} \\mathbf{c}$$\n",
    "\n",
    "- Giá trị nhỏ nhất:\n",
    "$$h(\\mathbf{x}^*) =b - \\tfrac12 \\mathbf{c}^\\top Q^{-1} \\mathbf{c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6da77a",
   "metadata": {},
   "source": [
    "#### 7. Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve_quadratic_momentum.py\n",
    "#\n",
    "#Giải bài toán tối ưu hàm bậc hai theo phong cách d2l.ai và chạy Momentum (d2l-style).\n",
    "\n",
    "#Nội dung tham chiếu gốc (markdown) đã được chuyển thành code và hàm tương ứng.\n",
    "#(Reference: user's uploaded markdown). :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "#Sử dụng: Có thể chỉnh Q, c, x0, beta, eta, steps\n",
    "\n",
    "#File sẽ:\n",
    "#- in nghiệm đóng (analytical) x_star và h(x_star)\n",
    "#- chạy thuật toán Momentum theo d2l:\n",
    "#    g_t = Q x_t + c\n",
    "#    v_{t+1} = beta * v_t + g_t\n",
    "#    x_{t+1} = x_t - eta * v_{t+1}\n",
    "#- xuất bảng tra cứu các bước ra CSV: momentum_steps.csv\n",
    "#- (tùy chọn) vẽ quỹ đạo x_t nếu matplotlib có sẵn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TÍNH GIÁ TRỊ HÀM\n",
    "def h_value(x, Q, c, b=0.0):\n",
    "    \"\"\"Compute h(x) = 1/2 x^T Q x + x^T c + b\"\"\"\n",
    "    x = np.atleast_1d(x)\n",
    "    return 0.5 * float(x.T @ (Q @ x)) + float(x.T @ c) + float(b)\n",
    "\n",
    "# TÌM NGHIỆM ĐÚNG\n",
    "def analytic_minimizer(Q, c):\n",
    "    \"\"\"Return analytic minimizer x* = -Q^{-1} c (if Q invertible)\"\"\"\n",
    "    Q = np.atleast_2d(Q)\n",
    "    c = np.atleast_1d(c)\n",
    "    if Q.shape[0] != Q.shape[1]:\n",
    "        raise ValueError(\"Q must be square\")\n",
    "    try:\n",
    "        Qinv = np.linalg.inv(Q)\n",
    "    except np.linalg.LinAlgError:\n",
    "        raise np.linalg.LinAlgError(\"Q is singular; inverse does not exist\")\n",
    "    x_star = - Qinv @ c\n",
    "    return x_star\n",
    "\n",
    "#THỰC HIỆN MOMENTUM\n",
    "def momentum_d2l(Q, c, x0, beta=0.9, eta=0.1, steps=50, b=0.0, verbose=False):\n",
    "    \"\"\"\n",
    "    Run d2l-style Momentum on quadratic h(x) = 1/2 x^T Q x + x^T c + b\n",
    "    - g_t = Q x_t + c\n",
    "    - v_{t+1} = beta v_t + g_t\n",
    "    - x_{t+1} = x_t - eta * v_{t+1}\n",
    "    Returns:\n",
    "      xs: list of x vectors from t=0..steps\n",
    "      vs: list of v vectors from t=0..steps (v_0 included)\n",
    "      gs: list of g vectors from t=0..steps-1\n",
    "    \"\"\"\n",
    "    #KHỞI TẠO\n",
    "    Q = np.atleast_2d(Q)\n",
    "    c = np.atleast_1d(c)\n",
    "    x = np.atleast_1d(x0).astype(float).copy()\n",
    "    n = Q.shape[0]\n",
    "    if x.shape[0] != n or c.shape[0] != n:\n",
    "        raise ValueError(\"Dimension mismatch: Q is %sx%s, x,c must be length %s\" % (n,n,n))\n",
    "\n",
    "    #TẠO DANH SÁCH ĐỂ LƯU QUÁ TRÌNH\n",
    "    xs = [x.copy()]\n",
    "    vs = [np.zeros(n)]\n",
    "    gs = []\n",
    "\n",
    "    v = np.zeros(n)\n",
    "    for t in range(steps):\n",
    "        g = Q @ x + c   # gradient at x_t\n",
    "        gs.append(g.copy())\n",
    "        v = beta * v + g\n",
    "        x = x - eta * v\n",
    "        vs.append(v.copy())\n",
    "        xs.append(x.copy())\n",
    "        if verbose:\n",
    "            print(f\"t={t:2d}: x={x}, v={v}, g={g}\")\n",
    "    return np.array(xs), np.array(vs), np.array(gs)\n",
    "\n",
    "#LƯU TOÀN BỘ QUỸ ĐẠO TỐI ƯU HÓA RA CSV\n",
    "def save_steps_csv(xs, vs, gs, filename=\"momentum_steps.csv\"):\n",
    "    \"\"\"Save trajectory to CSV with columns: t, x0, x1, v0, v1, g0, g1\"\"\"\n",
    "    rows = []\n",
    "    steps = xs.shape[0] - 1\n",
    "    for t in range(steps+1):\n",
    "        x_t = xs[t]\n",
    "        v_t = vs[t]\n",
    "        g_t = gs[t] if t < gs.shape[0] else np.array([np.nan]*x_t.size)\n",
    "        row = {\"t\": t}\n",
    "        for i in range(x_t.size):\n",
    "            row[f\"x_{i}\"] = float(x_t[i])\n",
    "            row[f\"v_{i}\"] = float(v_t[i]) if not np.isnan(v_t[i]) else \"\"\n",
    "            row[f\"g_{i}\"] = float(g_t[i]) if not np.isnan(g_t[i]) else \"\"\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filename, index=False)\n",
    "    return filename, df\n",
    "\n",
    "#HÀM DEMO\n",
    "def demo_example():\n",
    "    \"\"\"Run the example from the conversation (2x2 Q) and print results\"\"\"\n",
    "    Q = np.array([[3.0, 0.0], [0.0, 1.0]])\n",
    "    c = np.array([-6.0, 2.0])\n",
    "    b = 0.0\n",
    "\n",
    "    x0 = np.array([0.0, 0.0])\n",
    "    beta = 0.9\n",
    "    eta = 0.1\n",
    "    steps = 100\n",
    "\n",
    "    print(\"Analytic solution:\")\n",
    "    x_star = analytic_minimizer(Q, c)\n",
    "    print(\" x* =\", x_star)\n",
    "    print(\" h(x*) =\", h_value(x_star, Q, c, b))\n",
    "\n",
    "    print(\"\\nRunning d2l-style Momentum:\")\n",
    "    xs, vs, gs = momentum_d2l(Q, c, x0, beta=beta, eta=eta, steps=steps, b=b, verbose=False)\n",
    "\n",
    "    # Print a short table\n",
    "    for t in range(xs.shape[0]):\n",
    "        x_t = xs[t]\n",
    "        v_t = vs[t]\n",
    "        g_t = gs[t] if t < gs.shape[0] else np.array([np.nan, np.nan])\n",
    "        print(f\"t={t:2d} | x={x_t.round(6)} | v={v_t.round(6)} | g={np.round(g_t,6)} | h(x)={h_value(x_t, Q, c, b):.6f}\")\n",
    "\n",
    "    fname, df = save_steps_csv(xs, vs, gs, filename=\"momentum_steps.csv\")\n",
    "    print(f\"\\nSaved trajectory to {fname} (also returned as pandas DataFrame).\")\n",
    "    return xs, vs, gs, df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # If run as script, execute demo\n",
    "    xs, vs, gs, df = demo_example()\n",
    "    # Optionally: plot if matplotlib available\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        xs_arr = np.array(xs)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(xs_arr[:,0], xs_arr[:,1], '-o', label='x_t trajectory')\n",
    "        plt.scatter([xs_arr[-1,0]],[xs_arr[-1,1]], color='red', label='final')\n",
    "        plt.scatter([xs_arr[0,0]],[xs_arr[0,1]], color='green', label='start')\n",
    "        plt.xlabel('x[0]')\n",
    "        plt.ylabel('x[1]')\n",
    "        plt.title('Trajectory of x_t (Momentum d2l-style)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c22411",
   "metadata": {},
   "source": [
    "<!-- ######################################################################################################### -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02b880",
   "metadata": {},
   "source": [
    "### III.4. Bài 4\n",
    "What changes when we perform stochastic gradient descent with momentum? What happens when we use minibatch stochastic gradient descent with momentum? Experiment with the parameters?\n",
    "\n",
    "#### What changes when we perform stochastic gradient descent with momentum?\n",
    "##### Có thay đổi gì nếu ta sử dụng SGD với Momentum?\n",
    "\n",
    "\n",
    "Trước hết, cần phải nhắc lại Stochastic Gradient Descent (SGD) là gì?\n",
    "SGD thực ra là GD đã được cải tiến về hiệu năng.\n",
    "Nếu GD thông thường, hay gọi là Batch Gradient Descent, khi tính gradient của hàm mục tiêu tại $\\mathbf{x}$ sẽ là: <br>\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\mathbf{x}) \\tag{D2L 12.4.2}\n",
    "\\end{align}\n",
    "\n",
    "thì GD sẽ có độ phức tạp tính toán là O(n) cho mỗi iteration, nên SGD giúp giải quyết độ phức tạp tính toán này bằng cách: <br>\n",
    "1. Chọn ngẫu nhiên một mẫu dữ liệu từ vị trí $i \\in \\{1,...,n\\}$.\n",
    "2. Tính gradient của mẫu ngẫu nhiên đó để cập nhật $\\mathbf{x}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}) \\tag{D2L 12.4.3}\n",
    "\\end{align}\n",
    "\n",
    "Trong đó, người ta muốn nhấn mạnh rằng SGD $\\nabla f_i(\\mathbf{x})$ là một \"unbiased estimate\" của full GD.<br>\n",
    "Tuy nhiên, khi sử dụng SGD, người ta mong muốn có thể giảm được chi phí tính toán lớn trên toàn bộ điểm dữ liệu, nên có thể xem stochastic gradient gần đúng bằng kỳ vọng trung bình của toàn bộ gradient. <br>\n",
    "Ở đó, $\\nabla f_i(\\mathbf{x})$ còn được xem là điểm gây nhiễu mạnh vì chỉ tính trên một sample ngẫu nhiên, có thể gây ra sai số mạnh, vì ý tưởng của SGD coi **gradient ở mỗi bước** đều quan trọng như nhau.<br>\n",
    "Trong nhiều trường hợp bị nhiễu, loss function có thể sẽ hội tụ chậm hơn và di chuyển mạnh theo đường zig-zag.\n",
    "<br><br>\n",
    "Ví dụ: ta có chuỗi gradient như sau:\n",
    "\n",
    "| $\\nabla f_1$ | $\\nabla f_2$ | $\\nabla f_3$ | $\\nabla f_4$ | $\\nabla f_5$ |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 50 | 47 | 52 | **500** | 49 |\n",
    "| | | | **Nhiễu** | |\n",
    "\n",
    "Nên, nếu dùng với \"động lượng\" (momentum) - một động lượng tích lũy từ nhiều bước trước, giúp giảm dao động, và tăng tốc theo hướng có lợi. Mometum **trung bình hóa có trọng số theo thời gian**, nên nhiễu cao bất thường sẽ bị đè xuống bởi quá khứ.\n",
    "\n",
    "Công thức như sau: <br>\n",
    "\n",
    "$$ \\mathbf{v} \\leftarrow \\beta \\mathbf{v} + (1 - \\beta) \\nabla f_i(\\mathbf{x}) $$\n",
    "$$ \\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\mathbf{v} $$\n",
    "\n",
    "Trong trường hợp không chuẩn hóa momentum, hoặc đơn giản hơn, ta có:\n",
    "\n",
    "$$ \\mathbf{v} \\leftarrow \\beta \\mathbf{v} + \\nabla f_i(\\mathbf{x}) $$\n",
    "$$ \\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\mathbf{v} $$\n",
    "\n",
    "Thì với $\\beta = 0.9$, ta có: $v_4 = 0.9 v_3 + 500$, nhưng thực ra $v_3$ đã là lịch sử được làm mượt, tức là đã bao gồm các tính toán ở bước 2, bước 1,... nên nhiễu 500 sẽ không kéo toàn bộ $v_t$ lên quá cao.\n",
    "\n",
    "--\n",
    "\n",
    "Qua đó, có thể thấy, momentum ở đây mang cơ chế lọc nhiễu (**Moving Average**), chứ không khuếch đại nhiễu. <br>\n",
    "Nếu viết momentum theo dạng unfolded, ta có: <br>\n",
    "$v_t = g_t + \\beta g_{t-1} + \\beta^2 g_{t-2} + \\beta^3 g_{t-3} + ...$ <br>\n",
    "\n",
    "- Nhiễu 500 xuất hiện một lần, được nhân với trọng số 1 ($g_t$).\n",
    "- Trong khi 50, 47, 52 xuất hiện nhiều lần, với các trọng số $\\beta, \\beta^2, \\beta^3$.\n",
    "\n",
    "Vai trò của quá khứ lớn hơn vài trò của một sai số đơn lẻ.\n",
    "\n",
    "--\n",
    "\n",
    "Vậy nếu chỉ xét trong trường hợp SGD nhiễu mạnh, momentum giúp ổn định, giảm nhiểu, giảm tác động của những điểm gradient quá lớn, vì nó không dựa 100% vào gradient hiện tại mà dựa vào tích lũy quá khứ.\n",
    "\n",
    "~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0898c63",
   "metadata": {},
   "source": [
    "#### What happens when we use minibatch stochastic gradient descent with momentum?\n",
    "##### Có gì xảy ra khi sử dụng minibatch SGD với Momentum?\n",
    "\n",
    "Như đã đề cập, Stochastic Gradient Descent được thiết kế với ý tưởng xem các gradient quan trọng như nhau, và để giảm chi phí tính toán trên toàn bộ điểm dữ liệu như Full Gradient, thì SGD chỉ tính gradient tức thời tại một sample ngẫu nhiên.\n",
    "\n",
    "Tuy vậy, có thể sẽ gây ra nhiễu mạnh, nên một ý tưởng \"lai\" giữa full batch gradient với SGD gọi là **Minibatch SGD**, tức là: \n",
    "- thay vì chỉ tính gradient tức thời tại một điểm sample ngẫu nhiên,\n",
    "- cũng không tính trên toàn bộ điểm dữ liệu tránh chi phí tính toán lớn\n",
    "\n",
    "$\\rightarrow$ tính trung bình các gradient của một \"lô\" điểm ngẫu nhiên, ví dụ 2, 4, 8, 16, 32,... điểm ngẫu nhiên để vừa giảm chi phí tính toán (Full GD) và tránh nhiễu mạnh (SGD).\n",
    "\n",
    "Nếu chỉ xét trên Minibatch SGD, gradient đã ổn định hơn SGD thông thường nhiều. Tuy nhiên, đôi khi vẫn có thể bị ảnh hưởng với nhiễu nếu dùng batch nhỏ.\n",
    "\n",
    "Mở rộng với ví dụ ở phần trên:\n",
    "\n",
    "| $\\nabla f_1$ | $\\nabla f_2$ | $\\nabla f_3$ | $\\nabla f_4$ | $\\nabla f_5$ | $\\nabla f_6$ | $\\nabla f_7$ | $\\nabla f_8$ | $\\nabla f_9$ | $\\nabla f_{10}$ |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 50 | 47 | 52 | **500** | 49 | 48 | 50 | **400** | 47 | 51 |\n",
    "| | | | **Nhiễu** | | | | **Nhiễu** | | |\n",
    "\n",
    "Nếu dùng batch nhỏ như batch = 2 hoặc batch = 4, sample ngẫu nhiên đều rơi vào nhiễu (500, 400) thì hướng di chuyển sẽ có dao động mạnh, zig-zag nhiều, có điều sẽ \"mượt\" hơn SGD thuần nhiều.\n",
    "\n",
    "Về mặt toán học, Minibatch SGD sẽ lấy một mini-batch $\\mathcal B_t$ gồm $b$ mẫu ngẫu nhiên: <br>\n",
    "$$ \\mathbf{g}_t \\leftarrow \\frac{1}{\\mathcal B_t} \\sum_{i \\in \\mathcal B_t} \\nabla f_i(\\mathbf{x}_t) $$\n",
    "$$ \\mathbf{x}_{t+1} \\leftarrow \\mathbf{x}_t - \\eta \\mathbf{g}_t $$\n",
    "\n",
    "Nếu batch_size = 4, ví dụ chọn ngẫu nhiên $\\mathcal B_t = \\{1,4,7,8\\}$ <br>\n",
    "thì: $\\mathcal g_t = \\frac {50+500+50+400}{4} = 250$ <br>\n",
    "\n",
    "Nhận xét:\n",
    "- So với SGD nếu chọn ngẫu nhiên rơi vào index 4 hoặc 8, $\\mathbf g_t = 500$ hoặc $\\mathbf g_t = 400 \\rightarrow$ nhiễu mạnh.\n",
    "- Mini-batch SGD với batch_size=4 đã kéo nhiễu xuống còn 250.\n",
    "\n",
    "--\n",
    "\n",
    "Mini-batch SGD thêm Momentum, thêm một \"động lượng\" tích lũy:\n",
    "\n",
    "$$ \\mathbf v_t \\leftarrow \\beta \\mathbf v_{t-1} + \\mathbf g_t $$\n",
    "$$ \\mathbf{x}_{t+1} \\leftarrow \\mathbf{x}_t - \\eta \\mathbf{v}_t $$\n",
    "\n",
    "Như đã đề cập ở trên, momentum không làm khuếch đại nhiễu, mà giúp ổn định, kiềm nhiễu lại.<br>\n",
    "Kết hợp với mini-batch vốn đã giảm nhiễu nhờ trung bình hóa gradient rồi, momentum càng làm mượt đường di chuyển hơn.<br>\n",
    "Đồng thời, còn giúp tăng tốc theo hướng đúng, nhờ vận tốc tích lũy trong quá khứ, thay vì bị lệch bởi điểm bất thường.\n",
    "\n",
    "--\n",
    "\n",
    "Kết quả:\n",
    "- Minibatch SGD giảm phương sai ngắn hạn, momentum lọc dao động còn lại, làm mượt đường di chuyển hơn SGD thông thường.\n",
    "- Giảm zig-zag mạnh, hội tụ nhanh hơn.\n",
    "- Ổn định hơn trong các rãnh hẹp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4793c",
   "metadata": {},
   "source": [
    "#### Experiment with the parameters?\n",
    "\n",
    "Thực hành thí nghiệm với hàm số sau:\n",
    "\n",
    "$$ f(x) = x^2 + 10sin(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, b=10):\n",
    "    return x**2 + b * torch.sin(x)\n",
    "\n",
    "def gradient(x, b=10):\n",
    "    return 2 * x + b * torch.cos(x)\n",
    "\n",
    "# Plotting the loss function and its gradient\n",
    "x = torch.linspace(-10, 10, 400)\n",
    "y = loss_function(x)\n",
    "dy = gradient(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.numpy(), y.numpy(), label='Loss Function')\n",
    "plt.title('Loss Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x.numpy(), dy.numpy(), label='Gradient', color='orange')\n",
    "plt.title('Gradient of Loss Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Gradient')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b79416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x))\n",
    "\n",
    "# check convergence\n",
    "def has_converged(x_new, grad):\n",
    "    return np.linalg.norm(grad(x_new))/len(x_new) < 1e-3\n",
    "\n",
    "def gradient_descent(x_zero, learning_rate=0.01, max_iters=100):\n",
    "    x = torch.tensor(x_zero, requires_grad=True)\n",
    "    history = [x.item()]\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        loss = loss_function(x)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            x_new = x - learning_rate * x.grad\n",
    "        if has_converged(x_new.numpy(), lambda x: gradient(torch.tensor(x)).numpy()):\n",
    "            break\n",
    "        x = x_new.clone().detach().requires_grad_(True)\n",
    "        history.append(x.item())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "def stochastic_gradient_descent(x_zero, learning_rate=0.01, max_iters=100, batch_size=1):\n",
    "    x = torch.tensor(x_zero, requires_grad=True)\n",
    "    n = len(x_zero)\n",
    "    history = [x.item()]\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        indices = torch.randperm(n)[:batch_size]\n",
    "        x_batch = x[indices]\n",
    "        \n",
    "        loss = loss_function(x_batch).mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_new = x - learning_rate * x.grad\n",
    "        if has_converged(x_new.numpy(), lambda x: gradient(torch.tensor(x)).numpy()):\n",
    "            break\n",
    "        x = x_new.clone().detach().requires_grad_(True)\n",
    "        history.append(x.item())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "def sgd_momentum(x_zero, learning_rate=0.01, max_iters=100, batch_size=1, momentum=0.9):\n",
    "    x = torch.tensor(x_zero, requires_grad=True)\n",
    "    n = len(x_zero)\n",
    "    v = torch.zeros_like(x)\n",
    "    history = [x.item()]\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        indices = torch.randperm(n)[:batch_size]\n",
    "        x_batch = x[indices]\n",
    "        \n",
    "        loss = loss_function(x_batch).mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            v = momentum * v + x.grad\n",
    "            x_new = x - learning_rate * v\n",
    "        if has_converged(x_new.numpy(), lambda x: gradient(torch.tensor(x)).numpy()):\n",
    "            break\n",
    "        x = x_new.clone().detach().requires_grad_(True)\n",
    "        history.append(x.item())\n",
    "\n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plots(x_zero, x_gd, x_gd_history,\n",
    "               x_sgd, x_sgd_history,\n",
    "               x_sgd_momentum, x_sgd_momentum_history,\n",
    "               x_mini_sgd, x_mini_sgd_history,\n",
    "               x_mini_sgd_momentum, x_mini_sgd_momentum_history):\n",
    "    # Plot convergence histories on a main plot together\n",
    "    x = torch.linspace(-10, 10, 400)\n",
    "    y = loss_function(x)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x.numpy(), y.numpy(), label='Loss Function')\n",
    "    plt.title('Loss Function with Full Batch Gradient Descent')\n",
    "    plt.xlabel(f'x (Starting point: {x_zero})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot moving points for gradient descent history on the loss function plot\n",
    "    for i, x_val in enumerate(x_gd_history):\n",
    "        plt.scatter(x_val, loss_function(torch.tensor(x_val)).item(), color='r', s=50, alpha=0.6)\n",
    "        if i > 0:\n",
    "            plt.plot([x_gd_history[i-1], x_val],\n",
    "                    [loss_function(torch.tensor(x_gd_history[i-1])).item(),\n",
    "                    loss_function(torch.tensor(x_val)).item()],\n",
    "                    color='red')\n",
    "            \n",
    "     # Add starting and final points\n",
    "    plt.scatter(x_zero, loss_function(torch.tensor(x_zero)).item(), color='g', label='Starting Point', marker='*', s=150)\n",
    "    plt.scatter(x_gd.item(), loss_function(x_gd).item(), color='y', label='Final Point', marker='*', s=150)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    # Break #####################################################################################\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x.numpy(), y.numpy(), label='Loss Function')\n",
    "    plt.title('Loss Function with SGD')\n",
    "    plt.xlabel(f'x (Starting point: {x_zero})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "            \n",
    "    # Plot moving points for SGD history on the loss function plot\n",
    "    for i, x_val in enumerate(x_sgd_history):\n",
    "        plt.scatter(x_val, loss_function(torch.tensor(x_val)).item(), color='r', s=50, alpha=0.6)\n",
    "        if i > 0:\n",
    "            plt.plot([x_sgd_history[i-1], x_val],\n",
    "                    [loss_function(torch.tensor(x_sgd_history[i-1])).item(),\n",
    "                    loss_function(torch.tensor(x_val)).item()],\n",
    "                    color='r')\n",
    "            \n",
    "     # Add starting and final points\n",
    "    plt.scatter(x_zero, loss_function(torch.tensor(x_zero)).item(), color='g', label='Starting Point', marker='*', s=150)\n",
    "    plt.scatter(x_sgd.item(), loss_function(x_sgd).item(), color='y', label='Final Point', marker='*', s=150)\n",
    "    plt.legend()\n",
    "            \n",
    "    # Break\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x.numpy(), y.numpy(), label='Loss Function')\n",
    "    plt.title('Loss Function with SGD + Momentum')\n",
    "    plt.xlabel(f'x (Starting point: {x_zero})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot moving points for SGD with Momentum history on the loss function plot\n",
    "    for i, x_val in enumerate(x_sgd_momentum_history):\n",
    "        plt.scatter(x_val, loss_function(torch.tensor(x_val)).item(), color='orange', s=50, alpha=0.6)\n",
    "        if i > 0:\n",
    "            plt.plot([x_sgd_momentum_history[i-1], x_val],\n",
    "                    [loss_function(torch.tensor(x_sgd_momentum_history[i-1])).item(),\n",
    "                    loss_function(torch.tensor(x_val)).item()],\n",
    "                    color='orange')\n",
    "            \n",
    "     # Add starting and final points\n",
    "    plt.scatter(x_zero, loss_function(torch.tensor(x_zero)).item(), color='g', label='Starting Point', marker='*', s=150)\n",
    "    plt.scatter(x_sgd_momentum.item(), loss_function(x_sgd_momentum).item(), color='r', label='Final Point', marker='*', s=150)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    # # Break #####################################################################################\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x.numpy(), y.numpy(), label='Loss Function')\n",
    "    plt.title('Loss Function with Mini-batch SGD')\n",
    "    plt.xlabel(f'x (Starting point: {x_zero})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot moving points for Mini-batch SGD history on the loss function plot\n",
    "    for i, x_val in enumerate(x_mini_sgd_history):\n",
    "        plt.scatter(x_val, loss_function(torch.tensor(x_val)).item(), color='r', s=50, alpha=0.6)\n",
    "        if i > 0:\n",
    "            plt.plot([x_mini_sgd_history[i-1], x_val],\n",
    "                    [loss_function(torch.tensor(x_mini_sgd_history[i-1])).item(),\n",
    "                    loss_function(torch.tensor(x_val)).item()],\n",
    "                    color='r')\n",
    "            \n",
    "     # Add starting and final points\n",
    "    plt.scatter(x_zero, loss_function(torch.tensor(x_zero)).item(), color='g', label='Starting Point', marker='*', s=150)\n",
    "    plt.scatter(x_mini_sgd.item(), loss_function(x_mini_sgd).item(), color='y', label='Final Point', marker='*', s=150)\n",
    "    plt.legend()\n",
    "\n",
    "    # # Break\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x.numpy(), y.numpy(), label='Loss Function')\n",
    "    plt.title('Loss Function with Mini-batch SGD + Momentum')\n",
    "    plt.xlabel(f'x (Starting point: {x_zero})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "        \n",
    "    # Plot moving points for Mini-batch SGD with Momentum history on the loss function plot\n",
    "    for i, x_val in enumerate(x_mini_sgd_momentum_history):\n",
    "        plt.scatter(x_val, loss_function(torch.tensor(x_val)).item(), color='orange', s=50, alpha=0.6)\n",
    "        if i > 0:\n",
    "            plt.plot([x_mini_sgd_momentum_history[i-1], x_val],\n",
    "                    [loss_function(torch.tensor(x_mini_sgd_momentum_history[i-1])).item(),\n",
    "                    loss_function(torch.tensor(x_val)).item()],\n",
    "                    color='orange')\n",
    "            \n",
    "     # Add starting and final points\n",
    "    plt.scatter(x_zero, loss_function(torch.tensor(x_zero)).item(), color='g', label='Starting Point', marker='*', s=150)\n",
    "    plt.scatter(x_mini_sgd_momentum.item(), loss_function(x_mini_sgd_momentum).item(), color='r', label='Final Point', marker='*', s=150)\n",
    "    plt.legend()\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfde567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "\n",
    "def compute(x_zero=None, learning_rate=0.04, max_iters=1000, batch_size=10, momentum=0.9):\n",
    "  if x_zero is not None:\n",
    "    x_zero = x_zero\n",
    "  else:\n",
    "    x_zero = rnd.uniform(7, 10)\n",
    "  \n",
    "\n",
    "  print(x_zero)\n",
    "\n",
    "  x_gd, x_gd_history = gradient_descent([x_zero], learning_rate, max_iters)\n",
    "  x_sgd, x_sgd_history = stochastic_gradient_descent([x_zero], learning_rate, max_iters, 1)\n",
    "  x_mini_sgd, x_mini_sgd_history = stochastic_gradient_descent([x_zero], learning_rate, max_iters, batch_size)\n",
    "  x_sgd_momentum, x_sgd_momentum_history = sgd_momentum([x_zero], learning_rate, max_iters, 1, momentum)\n",
    "  x_mini_sgd_momentum, x_mini_sgd_momentum_history = sgd_momentum([x_zero], learning_rate, max_iters, batch_size, momentum)\n",
    "\n",
    "  print(f\"Gradient Descent Result: {x_gd.item()} after {len(x_gd_history)} iterations\")\n",
    "  print(f\"Stochastic Gradient Descent Result: {x_sgd.item()} after {len(x_sgd_history)} iterations\")\n",
    "  print(f\"Mini-batch Stochastic Gradient Descent Result: {x_mini_sgd.item()} after {len(x_mini_sgd_history)} iterations\")\n",
    "  print(f\"SGD with Momentum Result: {x_sgd_momentum.item()} after {len(x_sgd_momentum_history)} iterations\")\n",
    "  print(f\"Mini-batch SGD with Momentum Result: {x_mini_sgd_momentum.item()} after {len(x_mini_sgd_momentum_history)} iterations\")\n",
    "\n",
    "  draw_plots(x_zero, x_gd, x_gd_history,\n",
    "             x_sgd, x_sgd_history,\n",
    "             x_sgd_momentum, x_sgd_momentum_history,\n",
    "             x_mini_sgd, x_mini_sgd_history,\n",
    "             x_mini_sgd_momentum, x_mini_sgd_momentum_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9908d7b",
   "metadata": {},
   "source": [
    "Bắt đầu thí nghiệm bằng việc thay đổi các tham số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute(9.5, learning_rate=0.01, max_iters=1000, batch_size=10, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute(9.5, learning_rate=0.04, max_iters=1000, batch_size=10, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute(9.5, learning_rate=0.04, max_iters=1000, batch_size=10, momentum=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute(9.5, learning_rate=0.02, max_iters=1000, batch_size=5, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c40c5",
   "metadata": {},
   "source": [
    "### **IV. Bài tập mở rộng ngoài D2L**\n",
    "---\n",
    "\n",
    "### IV.0. **Quy ước**\n",
    "\n",
    "Trước khi thực hiện giải các bài toán tối ưu bằng các áp dụng các phương pháp **Gradient Descent và tối ưu với **Momentum**, cần tìm hiểu các quy ước chung về ký hiệu và công thức như sau:\n",
    "\n",
    "#### IV.0.1. Cấu trúc\n",
    "\n",
    "Vector tham số $(\\theta)$ bao gồm weights $(w)$ và bias $(b)$, ký thiệu như sau:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta = \n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\ w_{2} \\\\ w_{3} \\\\ b\n",
    "\\end{bmatrix}\n",
    "\\tag{5.0.1}\n",
    "\\end{align}\n",
    "\n",
    "#### IV.0.2. Công thức tính Momentum\n",
    "\n",
    "- Bước 1: Tính Gradient: $\\nabla \\mathbf{J} (\\theta_{t})$\n",
    "- Bước 2: Tính vận tốc (velocity): $\\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t + \\nabla \\mathbf{J} (\\theta_{t})$\n",
    "- Bước 3: Cập nhật tham số: $\\theta_{t+1} = \\theta_{t} - \\eta \\mathbf{v}_{t+1}$\n",
    "\n",
    "#### IV.0.3. Hằng số\n",
    "\n",
    "- Learning rate: $\\eta = 0.1$\n",
    "- Momentum factor: $\\beta = 0.9$\n",
    "- Khởi tạo vận tốc ban đầu: $\\mathbf{v}_{0} = [0,0,0,0]^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084fd9f",
   "metadata": {},
   "source": [
    "### IV.1. **Hồi quy tuyến tính đa biến (Multivariate Linear Regression)**\n",
    "\n",
    "**Mục tiêu**: Hiểu cách Momentum hoạt động với hàm loss MSE (L2 Norm) trên không gian nhiều chiều.\n",
    "\n",
    "#### IV.1.1. Thiết lập bài toán\n",
    "\n",
    "- Mô hình: \n",
    "  \n",
    "  \\begin{align}\n",
    "  f(x) = \\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b \\tag{5.1.1}\n",
    "  \\end{align}\n",
    "\n",
    "- Dữ liệu (một mẫu):\n",
    "  - Input: $\\mathbf{x} = [2, 1, -1]^\\top$\n",
    "  - Output thực tế: $y = 5$\n",
    "- Khởi tạo tham số:\n",
    "  $$ \\theta_{0} = [0, 0, 0, 0]^\\top = [\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3, b]^\\top $$\n",
    "- Hàm Loss:\n",
    "  \n",
    "  \\begin{align}\n",
    "  \\mathbf{J} = \\frac{1}{2}(\\hat{y}-y)^2 = \\frac{1}{2}(\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b - y)^2 \\tag{5.1.2}\n",
    "  \\end{align}\n",
    "\n",
    "- Tính gradient:\n",
    "  \n",
    "  \\begin{aligned}\n",
    "  \\nabla \\mathbf{J}(\\theta)\n",
    "\n",
    "  &= [\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_1}, \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_2}, \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_3}, \\frac{\\partial \\mathbf{J}}{\\partial b}]^\\top \\\\\n",
    "\n",
    "  &= [\\frac{\\partial [\\frac{1}{2}(\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b - y)^2]}{\\partial \\mathbf{w}_1}, ..., \\frac{\\partial [\\frac{1}{2}(\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b - y)^2]}{\\partial b}]^\\top \\\\\n",
    "\n",
    "  &= [(\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b - y)x_1, ..., (\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b - y)]^\\top \\\\\n",
    "\n",
    "  &= (\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b - y)[x_1, x_2, x_3, 1]^\\top \\\\\n",
    "\n",
    "  &= (\\hat{y} - y)[x_1, x_2, x_3, 1]^\\top\n",
    "\n",
    "  \\end{aligned}\n",
    "  \n",
    "Vậy:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla \\mathbf{J}(\\theta) = \\nabla \\mathbf{J}(\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3, b) = (\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b - y)[x_1, x_2, x_3, 1]^\\top \\tag{5.1.3}\n",
    "\\end{align}\n",
    "\n",
    "#### V.1.2. Giải chi tiết\n",
    "\n",
    "Vòng lặp 1, bước 1: Tính Forward và gradient\n",
    "\n",
    "- Dự đoán:\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\hat{y}_1 &= \\mathbf{w}_{1,0} x_1 + \\mathbf{w}_{2,0} x_2 + \\mathbf{w}_{3,0} x_3 + b \\\\\n",
    "  &= 0 \\cdot 2 + 0 \\cdot 1 + 0 \\cdot (-1) + 0 \\\\\n",
    "  &= 0\n",
    "  \\end{aligned}\n",
    "\n",
    "- Sai số: $hat{y}_1 - y = 0 - 5 = -5$\n",
    "- Gradient: $\\nabla \\mathbf{J}(\\theta_0) = (\\hat{y}_1 - y)[x_1, x_2, x_3, 1]^\\top = -5[2, 1, -1, 1]^\\top = [-10, -5, 5, -5]^\\top$\n",
    "\n",
    "Bước 2: Tính vận tốc $\\mathbf{v}_1$:\n",
    "\n",
    "$$ \\mathbf{v}_1 = \\beta \\mathbf{v}_0 + \\nabla \\mathbf{J}(\\theta_0) = 0.9[0,0,0,0]^\\top + [-10,-5,5,-5]^\\top = [-10,-5,5,-5]^\\top $$\n",
    "\n",
    "Bước 3: Cập nhật tham số $\\theta_1$:\n",
    "\n",
    "$$ \\theta_1 = \\theta_0 - \\eta \\mathbf{v}_1 = [0,0,0,0]^\\top - 0.1[-10,-5,5,-5]^\\top = [1, 0.5, -0.5, 0.5]^\\top $$\n",
    "\n",
    "Vòng lặp 2, bước 1: Tính Forward và gradient\n",
    "\n",
    "- Dự đoán:\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\hat{y}_2 &= \\mathbf{w}_{1,1} x_1 + \\mathbf{w}_{2,1} x_2 + \\mathbf{w}_{3,1} x_3 + b \\\\\n",
    "  &= 1 \\cdot 2 + 0.5 \\cdot 1 + (-0.5) \\cdot (-1) + 0.5 \\\\\n",
    "  &= 3.5\n",
    "  \\end{aligned}\n",
    "\n",
    "- Sai số: $hat{y}_2 - y = 3.5 - 5 = -1.5$\n",
    "- Gradient: $\\nabla \\mathbf{J}(\\theta_1) = (\\hat{y}_2 - y)[x_1, x_2, x_3, 1]^\\top = -1.5[2, 1, -1, 1]^\\top = [-3, -1.5, 1.5, -1.5]^\\top$\n",
    "\n",
    "Bước 2: Tính vận tốc $\\mathbf{v}_2$:\n",
    "\n",
    "$$ \\mathbf{v}_2 = \\beta \\mathbf{v}_1 + \\nabla \\mathbf{J}(\\theta_1) = 0.9[-10,-5,5,-5]^\\top + [-3,-1.5,1.5,-1.5]^\\top = [-12,-6,6,-6]^\\top $$\n",
    "\n",
    "Bước 3: Cập nhật tham số $\\theta_2$:\n",
    "\n",
    "$$ \\theta_2 = \\theta_1 - \\eta \\mathbf{v}_2 = [1,0.5,-0.5,0.5]^\\top - 0.1[-12,-6,6,-6]^\\top = [2.2, 1.1, -1.1, 1.1]^\\top $$\n",
    "\n",
    "Vòng lặp 3, bước 1:\n",
    "\n",
    "Tính $\\hat{y}_3$ \n",
    "\n",
    "\\begin{aligned}\n",
    "\\hat{y}_3 &= \\mathbf{w}_{1,2} x_1 + \\mathbf{w}_{2,2} x_2 + \\mathbf{w}_{3,2} x_3 + b \\\\\n",
    "&= 2.2 \\cdot 2 + 1.1 \\cdot 1 + (-1.1) \\cdot (-1) + 1.1 \\\\\n",
    "&= 7.7\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbd24f",
   "metadata": {},
   "source": [
    "### IV.2. **Logistic Regression (Bài toán phân loại)**\n",
    "\n",
    "**Mục tiêu**: Áp dụng Momentum với hàm phi tuyến Sigmoid và Loss Binary Cross Enropy.\n",
    "\n",
    "#### IV.2.1. Thiết lập bài toán\n",
    "\n",
    "- Mô hình:\n",
    "\n",
    "  \\begin{align}\n",
    "  \\mathbf{z} = \\mathbf{w}_{1} x_1 + \\mathbf{w}_{2} x_2 + \\mathbf{w}_{3} x_3 + b \n",
    "  \\Rightarrow a = \\sigma (\\mathbf{z}) = \\frac {1}{1+e^{-\\mathbf{z}}}\n",
    "  \\tag{5.2.1}\n",
    "  \\end{align}\n",
    "\n",
    "- Khởi tạo:  $ \\theta_{0} = [0, 0, 0, 0]^\\top = [\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3, b]^\\top $\n",
    "- Hàm Loss (Binary Cross Entropy): <br>\n",
    "  *Vì đây là bài toán phân loại nhị phân (0 hoặc 1), ta không dùng L2 mà dùng BCE.*\n",
    "\n",
    "  \\begin{align}\n",
    "  \\mathbf{J} = -[y \\cdot ln(a) + (1-y) \\cdot ln(1-a)]\n",
    "  \\tag{5.2.2}\n",
    "  \\end{align}\n",
    "\n",
    "  Thay $a=\\sigma (\\mathbf{w}_{1} x_1 + \\mathbf{w}_{2} x_2 + \\mathbf{w}_{3} x_3 + b)$ ta có hàm theo các biến $\\mathbf{w}, b$:\n",
    "\n",
    "  \\begin{align}\n",
    "  \\mathbf{J} (\\mathbf{w}, b) = -[y \\cdot ln(\\sigma (\\mathbf{z})) + (1-y) \\cdot ln(1-\\sigma (\\mathbf{z}))]\n",
    "  \\tag{5.2.3}\n",
    "  \\end{align}\n",
    "\n",
    "- Tính gradient:\n",
    "  \\begin{align}\n",
    "  \\nabla \\mathbf{J}(\\theta)\n",
    "  &= [\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_1}, \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_2}, \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_3}, \\frac{\\partial \\mathbf{J}}{\\partial b}]^\\top\n",
    "  \\tag{5.2.4}\n",
    "  \\end{align}\n",
    "\n",
    "  Sử dụng **Chain Rule** cho một biến $\\mathbf{w}_i$ bất kỳ, ta có chuỗi phụ thuộc: $\\mathbf{w}_i \\rightarrow \\mathbf{z} \\rightarrow a \\rightarrow \\mathbf{J}$.\n",
    "\n",
    "  $$ \\Rightarrow \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_i} = \n",
    "  \\frac{\\partial \\mathbf{J}}{\\partial a} \\times \n",
    "  \\frac{\\partial a}{\\partial \\mathbf{z}} \\times \n",
    "  \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_i} $$\n",
    "\n",
    "  - Bước 1: Tính $\\frac{\\partial \\mathbf{J}}{\\partial a}$\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\frac{\\partial \\mathbf{J}}{\\partial a}\n",
    "  &= \\frac{\\partial \\{-[y \\cdot ln(a) + (1-y) \\cdot ln(1-a)]\\}}{\\partial a} \\\\\n",
    "  &= -\\{ \\frac{\\partial [y \\cdot ln(a)]}{\\partial a} + \\frac{\\partial [(1-y)ln(1-a)]}{\\partial a} \\} \\\\\n",
    "  &= -[y \\cdot \\frac{1}{a} + (1-y) \\cdot (-1) \\cdot \\frac{1}{1-a}] \\\\\n",
    "  &= -\\frac{y}{a} + \\frac{1-y}{1-a} \\\\\n",
    "  &= \\frac{a-y}{a(1-a)} \\\\\n",
    "  \\end{aligned}\n",
    "\n",
    "  \\begin{align}\n",
    "  \\Rightarrow \\frac{\\partial \\mathbf{J}}{\\partial a} = \\frac{a-y}{a(1-a)} \n",
    "  \\tag{5.2.5}\n",
    "  \\end{align}\n",
    "\n",
    "  - Bước 2: Tính $\\frac{\\partial a}{\\partial \\mathbf{z}}$\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\frac{\\partial a}{\\partial \\mathbf{z}}\n",
    "  &= \\frac{\\partial[\\sigma(\\mathbf{z})]}{\\partial \\mathbf{z}} \\\\\n",
    "  &= \\frac{\\partial[ \\frac{1}{1+e^{-z}} ]}{\\partial \\mathbf{z}} \\\\\n",
    "  &= \\frac{\\partial[ \\frac{1}{1+\\frac{1}{e^z}} ]}{\\partial \\mathbf{z}} \\\\\n",
    "  &= \\frac{\\partial[ \\frac{1}{\\frac{e^z+1}{e^z}} ]}{\\partial \\mathbf{z}} \\\\\n",
    "  &= \\frac{\\partial[ \\frac{e^z}{1+e^z} ]}{\\partial \\mathbf{z}} \\\\\n",
    "  &= \\frac{(e^z)'(1+e^z) - e^z(1+e^z)'}{(1+e^z)^2} \\\\\n",
    "  &= \\frac{e^z (1+e^z)-e^z e^z}{(1+e^z)^2} \\\\\n",
    "  &= \\frac{e^z}{(1+e^z)^2}(1+e^z-e^z)    \\leftarrow xem lại  \\\\\n",
    "  &= \\frac{e^z}{1+e^z}(\\frac{1+e^z}{1+e^z} - \\frac{e^z}{1+e^z}) \\\\\n",
    "  &= \\sigma(\\mathbf{z})[1-\\sigma(\\mathbf{z})] \\\\\n",
    "  &= a \\cdot (1 - a)\n",
    "  \\end{aligned}\n",
    "\n",
    "  \\begin{align}\n",
    "  \\frac{\\partial a}{\\partial \\mathbf{z}} = a \\cdot (1 - a)\n",
    "  \\tag{5.2.6}\n",
    "  \\end{align}\n",
    "\n",
    "  - Bước 3: Tính $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_i}$\n",
    "\n",
    "  \\begin{aligned}\n",
    "  \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_i} = \\frac{\\partial(\\mathbf{w}_1 x_1 + \\mathbf{w}_2 x_2 + \\mathbf{w}_3 x_3 + b)}{\\partial \\mathbf{w}_i}\n",
    "   \\Rightarrow \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_1} = x_1; \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_2} = x_2; \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_3} = x_3; \\frac{\\partial \\mathbf{z}}{\\partial b} = 1;\n",
    "   \\tag{5.2.7}\n",
    "  \\end{aligned}\n",
    "\n",
    "Từ $(5.2.5)$, $(5.2.6)$, $(5.2.7)$ ta có:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\nabla \\mathbf{J}(\\theta)\n",
    "&= [\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_1}, \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_2}, \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{w}_3}, \\frac{\\partial \\mathbf{J}}{\\partial b}]^\\top \\\\\n",
    "&= [\n",
    "  \\frac{\\partial \\mathbf{J}}{\\partial a} \\times \n",
    "  \\frac{\\partial a}{\\partial \\mathbf{z}} \\times \n",
    "  \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_1},\n",
    "  \\frac{\\partial \\mathbf{J}}{\\partial a} \\times \n",
    "  \\frac{\\partial a}{\\partial \\mathbf{z}} \\times \n",
    "  \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_2},\n",
    "  \\frac{\\partial \\mathbf{J}}{\\partial a} \\times \n",
    "  \\frac{\\partial a}{\\partial \\mathbf{z}} \\times \n",
    "  \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_3},\n",
    "  \\frac{\\partial \\mathbf{J}}{\\partial a} \\times \n",
    "  \\frac{\\partial a}{\\partial \\mathbf{z}} \\times \n",
    "  \\frac{\\partial \\mathbf{z}}{\\partial b}\n",
    "]^\\top \\\\\n",
    "&= \\frac{\\partial \\mathbf{J}}{\\partial a} \\times \n",
    "   \\frac{\\partial a}{\\partial \\mathbf{z}} \\times\n",
    "   [\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_1}, \n",
    "   \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_2}, \n",
    "   \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{w}_3}, \n",
    "   \\frac{\\partial \\mathbf{z}}{\\partial b}]^\\top \\\\\n",
    "&= \\frac{a-y}{a(1-a)} \\cdot a \\cdot (1 - a) \\cdot [x_1, x_2, x_3, 1]^\\top \\\\\n",
    "&= (a-y) \\cdot [x_1, x_2, x_3, 1]^\\top\n",
    "\\end{aligned}\n",
    "\n",
    "Vậy: $\\nabla \\mathbf{J}(\\theta) = (a-y) \\cdot [x_1, x_2, x_3, 1]^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a62cc6-0341-4c5a-a38d-2e229fe15df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T04:44:26.825884Z",
     "iopub.status.busy": "2025-12-08T04:44:26.825126Z",
     "iopub.status.idle": "2025-12-08T04:44:26.863842Z",
     "shell.execute_reply": "2025-12-08T04:44:26.861876Z",
     "shell.execute_reply.started": "2025-12-08T04:44:26.825817Z"
    }
   },
   "source": [
    "## 2. Giải chi tiết\n",
    "\n",
    "### Vòng lặp 1:\n",
    "\n",
    "**Bước 1: Tính Forward & Gradient.**\n",
    "* **Forward:**\n",
    "$$z_1 = w_0 x_1 + w_0 x_2 + w_0 x_3 + b_0 = 0 \\times 1 + 0 \\times 2 + 0 \\times 1 + 0 = 0$$\n",
    "\n",
    "$$a_1 = \\sigma(z_1) = \\frac{1}{1+e^{-z_1}} = \\frac{1}{1+e^{-0}} = 0.5$$\n",
    "\n",
    "* **Gradient:**\n",
    "    $$\\nabla J(\\theta_0) = (a_1 - y)[x_1, x_2, x_3, 1]^T$$\n",
    "    $$= (0.5 - 1)[1, 2, 1, 1]^T = [-0.5; -1; -0.5; -0.5]^T$$\n",
    "\n",
    "**Bước 2: Tính vận tốc $\\mathbf{v}_1$**\n",
    "\n",
    "$$\\mathbf{v}_1 = \\beta \\mathbf{v}_0 + \\nabla J(\\theta_0) = 0.9[0, 0, 0, 0]^T + [-0.5; -1; -0.5; -0.5]^T$$\n",
    "$$= [-0.5; -1; -0.5; -0.5]^T$$\n",
    "\n",
    "**Bước 3: Cập nhật tham số $\\theta_1$**\n",
    "\n",
    "$$\\theta_1 = \\theta_0 - \\eta \\mathbf{v}_1 = [0, 0, 0, 0]^T - 0.1[-0.5; -1; -0.5; -0.5]^T$$\n",
    "$$= [0.05; 0.1; 0.05; 0.05]^T$$\n",
    "\n",
    "---\n",
    "\n",
    "### Vòng lặp 2:\n",
    "\n",
    "**Bước 1: Tính Forward & Gradient.**\n",
    "* **Forward:**\n",
    "$$z_2 = w_{1,1} x_1 + w_{2,1} x_2 + w_{3,1} x_3 + b_1$$\n",
    "$$= 0.05 \\times 1 + 0.1 \\times 2 + 0.05 \\times 1 + 0.05 = 0.35$$\n",
    "\n",
    "$$a_2 = \\sigma(z_2) = \\frac{1}{1+e^{-z_2}} = \\frac{1}{1+e^{-0.35}} = 0.587$$\n",
    "\n",
    "* **Gradient:**\n",
    "    $$\\nabla J(\\theta_1) = (a_2 - y)[x_1, x_2, x_3, 1]^T$$\n",
    "    $$= (0.587 - 1)[1, 2, 1, 1]^T = [-0.413; -0.826; -0.413; -0.413]^T$$\n",
    "\n",
    "**Bước 2: Tính vận tốc $\\mathbf{v}_2$**\n",
    "\n",
    "$$\\mathbf{v}_2 = \\beta \\mathbf{v}_1 + \\nabla J(\\theta_1)$$\n",
    "$$= 0.9[-0.5; -1; -0.5; -0.5]^T + [-0.413; -0.826; -0.413; -0.413]^T$$\n",
    "$$= [-0.863; -1.726; -0.863; -0.863]^T$$\n",
    "\n",
    "**Bước 3: Cập nhật tham số $\\theta_2$**\n",
    "\n",
    "$$\\theta_2 = \\theta_1 - \\eta \\mathbf{v}_2$$\n",
    "$$= [0.05; 0.1; 0.05; 0.05]^T - 0.1[-0.863; -1.726; -0.863; -0.863]^T$$\n",
    "$$= [0.1363; 0.2726; 0.1363; 0.1363]^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5019cf-c908-4f92-8bd5-218698eba80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
